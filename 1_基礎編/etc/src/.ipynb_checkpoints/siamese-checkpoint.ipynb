{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding,Input,Reshape,Subtract,Conv2D,MaxPool2D,Concatenate,Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings vectors\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = '../data/glove.6B.50d.txt'\n",
    "print('loading embeddings vectors')\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(' ')) for o in open(pretrained_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HyperParams# HyperP \n",
    "max_features = 20000   #学習に用いる単語数\n",
    "maxlen = 80  #学習に用いる最大長(これ以上は省略する)\n",
    "batch_size = 32\n",
    "embed_size = 50  #単語埋め込み次元数(今回は学習済みが50次元なので50次元)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeroPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrixの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDEX_FROM=3   \n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create embedding matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('create embedding matrix')\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "\n",
    "for word, i in word_to_id.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese networkの構造\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1200/1*XzVUiq-3lYFtZEW3XfmKqg.jpeg' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書のEncoder部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_size=16\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_encoder():\n",
    "    inputs =Input(shape=(maxlen,))\n",
    "    emb= Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inputs)  #Embedding層\n",
    "    reshape = Reshape((maxlen,embed_size,1))(emb)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "    out = Dense(out_size, activation='linear')(flatten)\n",
    " \n",
    "    encoder = Model(inputs, out)\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手法１ 通常のtextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "model_19 (Model)             (None, 16)                1020848   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,020,865\n",
      "Trainable params: 20,865\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = make_encoder()\n",
    "\n",
    "inp = Input((maxlen,))\n",
    "enc = encoder(inp)\n",
    "out = Dense(1,activation='sigmoid')(enc)\n",
    "model = Model(inp,out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 9s 365us/step - loss: 0.5207 - acc: 0.7336 - val_loss: 0.4448 - val_acc: 0.7882\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 8s 336us/step - loss: 0.4095 - acc: 0.8126 - val_loss: 0.4256 - val_acc: 0.8010\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 8s 335us/step - loss: 0.3622 - acc: 0.8392 - val_loss: 0.4387 - val_acc: 0.7966\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 8s 335us/step - loss: 0.3191 - acc: 0.8607 - val_loss: 0.4411 - val_acc: 0.8014\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 8s 335us/step - loss: 0.2863 - acc: 0.8789 - val_loss: 0.4561 - val_acc: 0.8020\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 8s 335us/step - loss: 0.2388 - acc: 0.9005 - val_loss: 0.5106 - val_acc: 0.7906\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 8s 336us/step - loss: 0.2058 - acc: 0.9186 - val_loss: 0.5612 - val_acc: 0.7878\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 8s 334us/step - loss: 0.1886 - acc: 0.9250 - val_loss: 0.6263 - val_acc: 0.7784\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 8s 335us/step - loss: 0.1509 - acc: 0.9437 - val_loss: 0.6501 - val_acc: 0.7863\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 8s 335us/step - loss: 0.1361 - acc: 0.9473 - val_loss: 0.7578 - val_acc: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19706b39b0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siameseネットワークの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp1 = Input(shape=(maxlen,))\n",
    "inp2 = Input(shape=(maxlen,))\n",
    "\n",
    "encoder1 = make_encoder()\n",
    "encoder2 = make_encoder()\n",
    "\n",
    "out1 = encoder1(inp1)\n",
    "out2 = encoder2(inp2)\n",
    "\n",
    "x =Concatenate()([out1,out2])\n",
    "\n",
    "pred = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model([inp1,inp2],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_34 (InputLayer)           (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_35 (InputLayer)           (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_24 (Model)                (None, 16)           1020848     input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_25 (Model)                (None, 16)           1020848     input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 32)           0           model_24[1][0]                   \n",
      "                                                                 model_25[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1)            33          concatenate_21[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,041,729\n",
      "Trainable params: 41,729\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train groups: [12500, 12500]\n",
      "test groups: [12500, 12500]\n"
     ]
    }
   ],
   "source": [
    "# reorganize by groups\n",
    "train_groups = [x_train[np.where(y_train==i)[0]] for i in np.unique(y_train)]\n",
    "test_groups = [x_test[np.where(y_test==i)[0]] for i in np.unique(y_train)]\n",
    "print('train groups:', [x.shape[0] for x in train_groups])\n",
    "print('test groups:', [x.shape[0] for x in test_groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_random_batch(in_groups, batch_halfsize = 8):\n",
    "    out_img_a, out_img_b, out_score = [], [], []\n",
    "    all_groups = list(range(len(in_groups)))\n",
    "    for match_group in [True, False]:\n",
    "        group_idx = np.random.choice(all_groups, size = batch_halfsize)\n",
    "        out_img_a += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in group_idx]\n",
    "        if match_group:\n",
    "            b_group_idx = group_idx\n",
    "            out_score += [1]*batch_halfsize\n",
    "        else:\n",
    "            # anything but the same group\n",
    "            non_group_idx = [np.random.choice([i for i in all_groups if i!=c_idx]) for c_idx in group_idx] \n",
    "            b_group_idx = non_group_idx\n",
    "            out_score += [0]*batch_halfsize\n",
    "            \n",
    "        out_img_b += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in b_group_idx]\n",
    "            \n",
    "    return np.stack(out_img_a,0), np.stack(out_img_b,0), np.stack(out_score,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_a, pv_b, pv_sim = gen_random_batch(train_groups, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元文章1 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> it is so rare that i get to rate a movie without having some reservation as to whether i should have gone up one or down one but this one did the explosion rate a notch higher or one down because my brain hurt trying to create a plot no this one yeah a solid no brainer one ten\n",
      "元文章2 the acting is below par it features a lot of really annoying rap music and poorly edited fight scenes on the plus side it's got that hispanic bloke in it who stars in every prison action thriller ever made and he shuts a door in this br br it's not very <UNK> but at least it's harmless br br if you were a massive fan of the original it's okay ish stuff br br if not you have been warned\n",
      "類似度 1\n",
      "-----------------------------\n",
      "元文章1 revenge on the <UNK> kids who wronged him plot will of course be familiar to those who've watched it before and those who've seen it before will probably watch it again those who are expecting ingmar bergman and will subsequently become <UNK> about their wasted time should just skip it marilyn manson on the soundtrack and david boreanaz denise richards and katherine <UNK> as eye candy go with the flow and enjoy it oh and i loved the creepy mask\n",
      "元文章2 the idea if anything <UNK> to the diversity and range of african themes with his film 500 years later it is a african issue but the idea doesn't fit that mold showing the artistic diversity the film is an all african cast but the topic is a human topic which most of us could relate to i just love the mild comedy in it and the <UNK> of <UNK> <UNK> is just amazing it is really a art house gem\n",
      "類似度 1\n",
      "-----------------------------\n",
      "元文章1 thanks to fine performances tight direction by john sturges the crisp monochrome cinematography of victor <UNK> and an atmospheric score by dimitri <UNK> extras however are no great shakes except for a radio version of jeopardy and trailers for both movies br br this disc is also part of a barbara stanwyck box set celebrating her <UNK> hard to believe that the lady would be over 100 years old if she was still around br br jeopardy an mgm winner\n",
      "元文章2 devastated br br improvisation might still be there though among all these wonderful performances near the end there's an unexpected scene where cassavetes and rowlands start talking non stop whether this was improvised or not is not something we have to wonder we have just got to watch and watching both of them exchanging life experiences and seeing words come truly alive in a conversation that means a lot more than what it <UNK> doesn't get more natural than that\n",
      "類似度 1\n",
      "-----------------------------\n",
      "元文章1 exquisite even the shabby ones br br the two young lads who play oliver and the artful dodger are wonderfully talented oliver reed does a great job portraying bill sykes to where you can't help but hope he comes to a terrible end which he does br br the dancing is cleverly choreographed and is mesmerizing oliver can hold its own with the likes of my fair lady the sound of music oklahoma etc a film for the entire family\n",
      "元文章2 bad but down right horrible and of no redeeming quality the plot was there one seemed to go no where the russians played silly kill or be killed games and the rest of the cast should be declared <UNK> and void for their pathetic performances i gave up about 3 4 of the way through and turned it off a 1 for awful only because there is nothing lower don't waste your time on this one you'll not miss anything\n",
      "類似度 0\n",
      "-----------------------------\n",
      "元文章1 eastwood or john wayne and more a three dimensional <UNK> his relationship with brennan is well played understated but nevertheless touching with a faint suggestion of george and lenny from of mice and men an altogether different type of western br br i certainly have more westerns to see but this is for now my favorite and the <UNK> by which i will necessarily judge all the others it deserves to be much better known and appreciated than it is\n",
      "元文章2 football teammates that he got diane pregnant in no way shape or form would a guy ever cheer another guy getting a girl pregnant in high school they might cheer about the guy having sex with the hot cheerleader but i can also guarantee that the first the football team heard about it would not be at a <UNK> br br it was obvious that this film didn't take itself so seriously and it wasn't hideously bad but come on\n",
      "類似度 0\n",
      "-----------------------------\n",
      "元文章1 it's packaged along with 1970's movies as metamorphosis is part of mill creek <UNK> 50 chilling classics there is basically no film quality difference whatsoever the final five minutes are pure bad movie cheese that actually for me at least save the movie from a lower rating pay attention to the computer terminology such as <UNK> <UNK> no wonder peter's experiment failed your computer can't spell this is worthy of a view followed by a trip to your local tavern\n",
      "元文章2 is pretty good movie robert culp was very good in the movie and was perfect for the part its hard to believe that this is a true story but what can you do when i watched this i thought why do they have to do all of those things it isn't right but they learned their lesson when they picked on the wrong man anyway if you ever see this movie on tv watch it because its a good one\n",
      "類似度 0\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for line1,line2,sim in zip(pv_a,pv_b,pv_sim):\n",
    "    print('元文章1',' '.join(id_to_word[id] for id in line1))\n",
    "    print('元文章2',' '.join(id_to_word[id] for id in line2))\n",
    "    print('類似度',sim)\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 60s 119ms/step - loss: 0.7049 - acc: 0.5006 - val_loss: 0.7013 - val_acc: 0.4971\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 52s 105ms/step - loss: 0.6988 - acc: 0.4953 - val_loss: 0.6998 - val_acc: 0.4980\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.6960 - acc: 0.5001 - val_loss: 0.6965 - val_acc: 0.4995\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.6959 - acc: 0.5003 - val_loss: 0.6955 - val_acc: 0.4985\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 53s 105ms/step - loss: 0.6960 - acc: 0.4949 - val_loss: 0.6964 - val_acc: 0.4893\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.6959 - acc: 0.4989 - val_loss: 0.6940 - val_acc: 0.4956\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 53s 105ms/step - loss: 0.6955 - acc: 0.4951 - val_loss: 0.6972 - val_acc: 0.5117\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 53s 105ms/step - loss: 0.6957 - acc: 0.4945 - val_loss: 0.6971 - val_acc: 0.4941\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 52s 105ms/step - loss: 0.6952 - acc: 0.4974 - val_loss: 0.6954 - val_acc: 0.5024\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.6957 - acc: 0.4970 - val_loss: 0.6959 - val_acc: 0.4966\n"
     ]
    }
   ],
   "source": [
    "# make a generator out of the data\n",
    "def siam_gen(in_groups, batch_size = 32):\n",
    "    while True:\n",
    "        pv_a, pv_b, pv_sim = gen_random_batch(train_groups, batch_size//2)\n",
    "        yield [pv_a, pv_b], pv_sim\n",
    "        \n",
    "\n",
    "valid_a, valid_b, valid_sim = gen_random_batch(test_groups, 1024)\n",
    "loss_history = model.fit_generator(siam_gen(train_groups), \n",
    "                                                            steps_per_epoch = 500,\n",
    "                                                            validation_data=([valid_a, valid_b], valid_sim),\n",
    "                                                            epochs = 10,\n",
    "                                                            verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
