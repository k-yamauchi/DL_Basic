{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN) Encoder-Decoderモデルで英日翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データセットの読み込みと単語・品詞のID化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. データセットについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.enとtrain.jaの中身は次のようになっています.\n",
    "\n",
    "train.enの中身 (英語の文)\n",
    "```\n",
    "i can 't tell who will arrive first .\n",
    "many animals have been destroyed by men .\n",
    "i 'm in the tennis club .\n",
    "︙\n",
    "```\n",
    "\n",
    "train.jaの中身(日本語の文, 対訳)\n",
    "```\n",
    "誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n",
    "多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n",
    "私 は テニス 部員 で す 。\n",
    "︙\n",
    "```\n",
    "(データセットにはTanaka Corpus ( http://www.edrdg.org/wiki/index.php/Tanaka_Corpus )の一部を抽出した \n",
    "small_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods ( https://github.com/odashi/small_parallel_enja ) を使っています.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. 単語・品詞のID化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のままだと扱いづらいので, それぞれの単語をIDに置き換えます. 以下のコードでは, まず`build_vocab`で単語->idの辞書(`w2i`)を作り, それを元に`encode`で各単語をid化しています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(file_path):\n",
    "    vocab = set()\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()\n",
    "        vocab.update(words)\n",
    "\n",
    "    w2i = {w: np.int32(i+3) for i, w in enumerate(vocab)}   #<pad>を0,<s>を１,</s>を２にするため3からスタート\n",
    "    w2i['<s>'], w2i['</s>'] = np.int32(1), np.int32(2) # 文の先頭<s>は1・終端記号</s>は2\n",
    "    w2i['<pad>'] = np.int32(0)  \n",
    "\n",
    "    return w2i\n",
    "\n",
    "def encode(sentence, w2i):\n",
    "    encoded_sentence = []\n",
    "    for w in sentence:\n",
    "        encoded_sentence.append(w2i[w])\n",
    "    return encoded_sentence\n",
    "\n",
    "def load_data(file_path, vocab=None, w2i=None):\n",
    "    if vocab is None and w2i is None:\n",
    "        w2i = build_vocab(file_path)\n",
    "    \n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        s = line.strip().split()\n",
    "        s = ['<s>'] + s + ['</s>']\n",
    "        enc = encode(s, w2i)\n",
    "        data.append(enc)\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "    return data, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 英語->日本語\n",
    "\n",
    "train_X, e_w2i, e_i2w = load_data('train.en')\n",
    "train_y, j_w2i, j_i2w = load_data('train.ja')\n",
    "\n",
    "train_X, _, train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42) # 演習用に縮小\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.02, random_state=42)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 611, 5058, 4584, 593, 6314, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "彼\n",
      "ら\n",
      "は\n",
      "私\n",
      "を\n",
      "ボブ\n",
      "と\n",
      "呼\n",
      "び\n",
      "ま\n",
      "す\n",
      "。\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for w in train_y[0]:\n",
    "    print(j_i2w[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: '証明',\n",
       " 4: '〔',\n",
       " 5: '提案',\n",
       " 6: '健康',\n",
       " 7: '過労',\n",
       " 8: 'ままごと',\n",
       " 9: '換え',\n",
       " 10: '含',\n",
       " 11: '１３',\n",
       " 12: 'つけ加え',\n",
       " 13: 'イヤ',\n",
       " 14: 'やし',\n",
       " 15: 'なまね',\n",
       " 16: 'じゃ',\n",
       " 17: '偏見',\n",
       " 18: '夏私',\n",
       " 19: '係',\n",
       " 20: 'あきあき',\n",
       " 21: '応対',\n",
       " 22: '毎晩',\n",
       " 23: '間に合',\n",
       " 24: '活発',\n",
       " 25: '売り切れ',\n",
       " 26: 'のせ',\n",
       " 27: '風',\n",
       " 28: '来次第',\n",
       " 29: '写真',\n",
       " 30: '絵',\n",
       " 31: 'つぶし',\n",
       " 32: '視力',\n",
       " 33: '覚め',\n",
       " 34: 'あいだ',\n",
       " 35: '返答',\n",
       " 36: '猛犬',\n",
       " 37: '偉',\n",
       " 38: '誤',\n",
       " 39: '包み',\n",
       " 40: '不足',\n",
       " 41: 'マリー',\n",
       " 42: '布',\n",
       " 43: '厳格',\n",
       " 44: 'じっくり',\n",
       " 45: 'ざあっと',\n",
       " 46: '夏北海道',\n",
       " 47: '冷房',\n",
       " 48: '本社',\n",
       " 49: '浅',\n",
       " 50: '羨望',\n",
       " 51: 'かゆ',\n",
       " 52: '楽しみ',\n",
       " 53: '不思議',\n",
       " 54: '苦労',\n",
       " 55: 'どなた',\n",
       " 56: '８',\n",
       " 57: '色盲',\n",
       " 58: '白目',\n",
       " 59: '・',\n",
       " 60: 'アメリカ',\n",
       " 61: '暴露',\n",
       " 62: 'オフレコ',\n",
       " 63: '守り',\n",
       " 64: 'パソコン',\n",
       " 65: '操縦',\n",
       " 66: 'たこ',\n",
       " 67: '月曜',\n",
       " 68: '哀',\n",
       " 69: '引き渡',\n",
       " 70: '人混み',\n",
       " 71: '取り決め',\n",
       " 72: '酔っぱら',\n",
       " 73: '手間',\n",
       " 74: 'そう',\n",
       " 75: '願',\n",
       " 76: 'とげ',\n",
       " 77: '断言',\n",
       " 78: '冊',\n",
       " 79: '身の毛',\n",
       " 80: 'ひきかけ',\n",
       " 81: 'たずね',\n",
       " 82: '役割',\n",
       " 83: '病名',\n",
       " 84: 'ユミ',\n",
       " 85: '皿洗い',\n",
       " 86: 'くらい',\n",
       " 87: '取り付',\n",
       " 88: '陸',\n",
       " 89: '継',\n",
       " 90: 'ステラ',\n",
       " 91: '対比',\n",
       " 92: '校舎',\n",
       " 93: 'とりかえ',\n",
       " 94: '追い込',\n",
       " 95: 'ハンバーガー',\n",
       " 96: '回想',\n",
       " 97: '掲示',\n",
       " 98: '故意',\n",
       " 99: '取扱',\n",
       " 100: '劣',\n",
       " 101: '両手',\n",
       " 102: 'すぐ',\n",
       " 103: '化学',\n",
       " 104: '人通り',\n",
       " 105: 'つきあ',\n",
       " 106: '恵子',\n",
       " 107: 'ぎめ',\n",
       " 108: '夜泊ま',\n",
       " 109: 'シンガポール',\n",
       " 110: '春',\n",
       " 111: 'にしか',\n",
       " 112: '忍耐',\n",
       " 113: 'おそれ',\n",
       " 114: '乾期',\n",
       " 115: 'ｙｏｕ',\n",
       " 116: '虜',\n",
       " 117: '方',\n",
       " 118: '旅館',\n",
       " 119: 'うつくし',\n",
       " 120: '言及',\n",
       " 121: '非',\n",
       " 122: '親離れ',\n",
       " 123: '１００',\n",
       " 124: '七',\n",
       " 125: '見掛け',\n",
       " 126: '朝発',\n",
       " 127: '女子',\n",
       " 128: 'ｓｆ',\n",
       " 129: '済み',\n",
       " 130: '古く',\n",
       " 131: '逃が',\n",
       " 132: '首',\n",
       " 133: '青森',\n",
       " 134: '政治',\n",
       " 135: '漢字',\n",
       " 136: 'クリーム',\n",
       " 137: 'バナナ',\n",
       " 138: '尽く',\n",
       " 139: '自営業',\n",
       " 140: 'ゆう',\n",
       " 141: '歌声',\n",
       " 142: 'ぐ',\n",
       " 143: '妻',\n",
       " 144: 'のびのび',\n",
       " 145: '召しあが',\n",
       " 146: 'とりなお',\n",
       " 147: '非番',\n",
       " 148: '怒り',\n",
       " 149: '広が',\n",
       " 150: '復讐',\n",
       " 151: '〜',\n",
       " 152: '討論',\n",
       " 153: '信念',\n",
       " 154: '割り切れ',\n",
       " 155: 'オーバー',\n",
       " 156: '達者',\n",
       " 157: '尊重',\n",
       " 158: '溶か',\n",
       " 159: '手ごろ',\n",
       " 160: '歯',\n",
       " 161: 'きのう',\n",
       " 162: '重視',\n",
       " 163: '入試',\n",
       " 164: '生計',\n",
       " 165: '心変わり',\n",
       " 166: 'ヶ月',\n",
       " 167: 'そそっかし',\n",
       " 168: 'どん底',\n",
       " 169: '叔母',\n",
       " 170: '扇風',\n",
       " 171: 'ふざけ',\n",
       " 172: 'こう',\n",
       " 173: 'メイキング',\n",
       " 174: '代金',\n",
       " 175: '建て',\n",
       " 176: 'タクシー',\n",
       " 177: 'イサオ',\n",
       " 178: '日焼け',\n",
       " 179: '釣',\n",
       " 180: 'クッキー',\n",
       " 181: 'くじけ',\n",
       " 182: '口頭',\n",
       " 183: 'ち',\n",
       " 184: 'おけ',\n",
       " 185: 'なんと',\n",
       " 186: '早退',\n",
       " 187: '夕食',\n",
       " 188: '上',\n",
       " 189: '餌',\n",
       " 190: '錆び',\n",
       " 191: '合わせ',\n",
       " 192: 'フェア',\n",
       " 193: '立派',\n",
       " 194: '魚釣り',\n",
       " 195: '核',\n",
       " 196: '海岸',\n",
       " 197: 'ここの',\n",
       " 198: '猿',\n",
       " 199: 'め',\n",
       " 200: 'って',\n",
       " 201: '見つか',\n",
       " 202: 'つらく',\n",
       " 203: '実業',\n",
       " 204: '取り返し',\n",
       " 205: '実家',\n",
       " 206: '油',\n",
       " 207: '締めつけ',\n",
       " 208: '打ち負か',\n",
       " 209: '初',\n",
       " 210: '気温',\n",
       " 211: '尊',\n",
       " 212: '敗れ',\n",
       " 213: '内気',\n",
       " 214: '間柄',\n",
       " 215: '心地',\n",
       " 216: '敵対',\n",
       " 217: 'さくや',\n",
       " 218: '指輪',\n",
       " 219: '謝礼',\n",
       " 220: '昨年',\n",
       " 221: '尋ね',\n",
       " 222: '教養',\n",
       " 223: 'もら',\n",
       " 224: 'ちがい',\n",
       " 225: '由利',\n",
       " 226: 'キャンセル',\n",
       " 227: 'まごい',\n",
       " 228: '撮影',\n",
       " 229: 'ぞ',\n",
       " 230: 'うま',\n",
       " 231: '半ば',\n",
       " 232: '昨日',\n",
       " 233: 'ほうび',\n",
       " 234: '鳴',\n",
       " 235: '旅',\n",
       " 236: '業界',\n",
       " 237: 'かが',\n",
       " 238: 'いわゆる',\n",
       " 239: '風通し',\n",
       " 240: 'スタッフ',\n",
       " 241: '費用',\n",
       " 242: '息づま',\n",
       " 243: '自発',\n",
       " 244: '首席',\n",
       " 245: 'ラジオ',\n",
       " 246: '良さ',\n",
       " 247: '生きれ',\n",
       " 248: '部下',\n",
       " 249: 'ながめ',\n",
       " 250: '真後ろ',\n",
       " 251: '引き起こ',\n",
       " 252: '一家',\n",
       " 253: '言葉添え',\n",
       " 254: 'いたわり',\n",
       " 255: '取り消',\n",
       " 256: 'イヤホン',\n",
       " 257: 'ほほえ',\n",
       " 258: '緻密',\n",
       " 259: '節約',\n",
       " 260: '筋',\n",
       " 261: 'シングル',\n",
       " 262: 'サイン',\n",
       " 263: '冬雪',\n",
       " 264: 'ちや',\n",
       " 265: '成績',\n",
       " 266: '描',\n",
       " 267: '装',\n",
       " 268: '年間',\n",
       " 269: '挨拶',\n",
       " 270: '５',\n",
       " 271: '口答え',\n",
       " 272: '不安',\n",
       " 273: 'スーツ',\n",
       " 274: 'たのし',\n",
       " 275: '被害',\n",
       " 276: 'ふりかえ',\n",
       " 277: 'メンバー',\n",
       " 278: '率直',\n",
       " 279: '賛成',\n",
       " 280: 'かだれ',\n",
       " 281: '運動',\n",
       " 282: 'つまら',\n",
       " 283: '通れ',\n",
       " 284: 'だか',\n",
       " 285: '表現',\n",
       " 286: '援助',\n",
       " 287: '地球',\n",
       " 288: '田中',\n",
       " 289: '寸断',\n",
       " 290: 'つかえ',\n",
       " 291: '野菜',\n",
       " 292: 'いたずら',\n",
       " 293: '結論',\n",
       " 294: 'ファックス',\n",
       " 295: '景色',\n",
       " 296: 'ガイド',\n",
       " 297: '球場',\n",
       " 298: '今朝食',\n",
       " 299: '症状',\n",
       " 300: '級友',\n",
       " 301: 'そっと',\n",
       " 302: '突然',\n",
       " 303: '愛嬌',\n",
       " 304: '能力',\n",
       " 305: 'ふさわし',\n",
       " 306: '演じ',\n",
       " 307: 'フジモリ',\n",
       " 308: 'かがめ',\n",
       " 309: '口数',\n",
       " 310: 'ぞんじ',\n",
       " 311: 'はつの',\n",
       " 312: '腰痛',\n",
       " 313: 'ねた',\n",
       " 314: '金',\n",
       " 315: '消し',\n",
       " 316: '油断',\n",
       " 317: '軽井沢',\n",
       " 318: 'たほう',\n",
       " 319: '言伝',\n",
       " 320: '以外',\n",
       " 321: '本来',\n",
       " 322: '生命',\n",
       " 323: '１５０',\n",
       " 324: '思え',\n",
       " 325: '失敗',\n",
       " 326: 'たたか',\n",
       " 327: '見舞い',\n",
       " 328: '亡',\n",
       " 329: '農夫',\n",
       " 330: '調査',\n",
       " 331: 'カンニング',\n",
       " 332: 'たのみ',\n",
       " 333: 'トッピング',\n",
       " 334: 'ためし',\n",
       " 335: 'いつごろ',\n",
       " 336: '国民',\n",
       " 337: '減ら',\n",
       " 338: '公園',\n",
       " 339: '知恵',\n",
       " 340: '子ども',\n",
       " 341: '匹',\n",
       " 342: 'パン',\n",
       " 343: 'ベテラン',\n",
       " 344: '登',\n",
       " 345: '前兆',\n",
       " 346: '逃走',\n",
       " 347: 'じい',\n",
       " 348: 'こつ',\n",
       " 349: 'カナダ',\n",
       " 350: 'すら',\n",
       " 351: 'うたが',\n",
       " 352: 'すっきり',\n",
       " 353: '国外',\n",
       " 354: '客',\n",
       " 355: 'なれ',\n",
       " 356: '如',\n",
       " 357: '前途',\n",
       " 358: '行儀',\n",
       " 359: '気分',\n",
       " 360: 'ちょうしょく',\n",
       " 361: 'ああまた',\n",
       " 362: '卑劣',\n",
       " 363: '迫',\n",
       " 364: '学科',\n",
       " 365: 'めんくら',\n",
       " 366: '合図',\n",
       " 367: 'あり',\n",
       " 368: '鱗',\n",
       " 369: '理解',\n",
       " 370: 'あした',\n",
       " 371: '生まれ',\n",
       " 372: '学外',\n",
       " 373: '１０',\n",
       " 374: '戦い',\n",
       " 375: 'ぎりぎり',\n",
       " 376: '心',\n",
       " 377: '掘り出',\n",
       " 378: '胸焼け',\n",
       " 379: '毎回',\n",
       " 380: '遅く',\n",
       " 381: '同様',\n",
       " 382: '街',\n",
       " 383: '会社',\n",
       " 384: '骨',\n",
       " 385: '年老い',\n",
       " 386: '誉め',\n",
       " 387: 'いそう',\n",
       " 388: '上流',\n",
       " 389: '遅れ',\n",
       " 390: 'ぬる',\n",
       " 391: '贈り物',\n",
       " 392: 'ニッケル',\n",
       " 393: '雑然',\n",
       " 394: 'レコード',\n",
       " 395: '醜',\n",
       " 396: '頂上',\n",
       " 397: 'ともかく',\n",
       " 398: 'せかせか',\n",
       " 399: 'はれ',\n",
       " 400: 'きれ',\n",
       " 401: '頂',\n",
       " 402: '肯定',\n",
       " 403: '文夫',\n",
       " 404: '突っ込',\n",
       " 405: 'わり',\n",
       " 406: '便り',\n",
       " 407: '１０００',\n",
       " 408: 'にらみつけ',\n",
       " 409: 'デビー',\n",
       " 410: 'アレックス',\n",
       " 411: 'ひろ',\n",
       " 412: '大差',\n",
       " 413: 'おもちゃ',\n",
       " 414: '妨げ',\n",
       " 415: 'できあが',\n",
       " 416: '横断',\n",
       " 417: '決まり',\n",
       " 418: '禁煙',\n",
       " 419: '言',\n",
       " 420: 'かわ',\n",
       " 421: 'おろか',\n",
       " 422: 'ちょっと',\n",
       " 423: '集まり',\n",
       " 424: 'くさ',\n",
       " 425: '代用',\n",
       " 426: '裏切り',\n",
       " 427: '雑魚',\n",
       " 428: '一帯',\n",
       " 429: 'はし',\n",
       " 430: 'とりかか',\n",
       " 431: '人物',\n",
       " 432: '取り出',\n",
       " 433: '抜け',\n",
       " 434: '校庭',\n",
       " 435: '明朝',\n",
       " 436: '四季',\n",
       " 437: 'かまい',\n",
       " 438: '朝一夕',\n",
       " 439: '顔色',\n",
       " 440: 'しぼ',\n",
       " 441: '悲惨',\n",
       " 442: 'しゃべれ',\n",
       " 443: '仙台',\n",
       " 444: '限界',\n",
       " 445: '悪名',\n",
       " 446: 'ほとり',\n",
       " 447: 'ども',\n",
       " 448: '化粧',\n",
       " 449: '容貌',\n",
       " 450: '物思い',\n",
       " 451: '蜜蜂',\n",
       " 452: '支払',\n",
       " 453: '天下',\n",
       " 454: 'やれやれ',\n",
       " 455: 'おいで',\n",
       " 456: '引き裂',\n",
       " 457: '辞儀',\n",
       " 458: '達',\n",
       " 459: 'ラブ',\n",
       " 460: '辞職',\n",
       " 461: '翻訳',\n",
       " 462: '商',\n",
       " 463: '普段',\n",
       " 464: '胎動',\n",
       " 465: '吹雪',\n",
       " 466: '扱え',\n",
       " 467: '仕方',\n",
       " 468: '自転',\n",
       " 469: '客入り',\n",
       " 470: '野原',\n",
       " 471: '暗記',\n",
       " 472: '大した',\n",
       " 473: '対岸',\n",
       " 474: '金星',\n",
       " 475: 'おんぶ',\n",
       " 476: 'ひより',\n",
       " 477: 'やきもき',\n",
       " 478: '返却',\n",
       " 479: '手放',\n",
       " 480: '雷',\n",
       " 481: '条件',\n",
       " 482: '想像',\n",
       " 483: '老婆',\n",
       " 484: '原宿',\n",
       " 485: '飛び出',\n",
       " 486: 'さびし',\n",
       " 487: '長び',\n",
       " 488: '直ぐ',\n",
       " 489: '呼び入れ',\n",
       " 490: '作家',\n",
       " 491: '混雑',\n",
       " 492: 'トマト',\n",
       " 493: '破たん',\n",
       " 494: 'たいてい',\n",
       " 495: '直前',\n",
       " 496: 'バレーボール',\n",
       " 497: 'ハイウエイ',\n",
       " 498: 'ホノルル',\n",
       " 499: '」',\n",
       " 500: '止め',\n",
       " 501: '淋し',\n",
       " 502: '一番',\n",
       " 503: 'すわ',\n",
       " 504: '身投げ',\n",
       " 505: 'じゅう',\n",
       " 506: '新品',\n",
       " 507: '不意',\n",
       " 508: '立た',\n",
       " 509: '割り込',\n",
       " 510: 'ダン',\n",
       " 511: 'ひな鳥',\n",
       " 512: '返',\n",
       " 513: '多量',\n",
       " 514: '間歩',\n",
       " 515: '雪',\n",
       " 516: '摂',\n",
       " 517: '魚屋',\n",
       " 518: 'どけな',\n",
       " 519: 'さしせま',\n",
       " 520: '木',\n",
       " 521: 'パッと',\n",
       " 522: 'つな',\n",
       " 523: '快楽',\n",
       " 524: '上出来',\n",
       " 525: 'スレーター',\n",
       " 526: '欲し',\n",
       " 527: '手前',\n",
       " 528: '領分',\n",
       " 529: '音信',\n",
       " 530: '知的',\n",
       " 531: '哀れ',\n",
       " 532: 'においで',\n",
       " 533: '豊富',\n",
       " 534: '賜物',\n",
       " 535: '傾向',\n",
       " 536: '回数',\n",
       " 537: '面前',\n",
       " 538: 'タイプ',\n",
       " 539: '文章',\n",
       " 540: '永久',\n",
       " 541: 'てがみ',\n",
       " 542: '外',\n",
       " 543: '優れ',\n",
       " 544: 'みさこ',\n",
       " 545: '事欠',\n",
       " 546: '姉妹',\n",
       " 547: '今持',\n",
       " 548: '運転',\n",
       " 549: 'さけ',\n",
       " 550: '接待',\n",
       " 551: '何語',\n",
       " 552: '病気',\n",
       " 553: 'け',\n",
       " 554: 'ひど',\n",
       " 555: '大金持ち',\n",
       " 556: 'かぎ',\n",
       " 557: '汽船',\n",
       " 558: '香り',\n",
       " 559: '経過',\n",
       " 560: '大いに',\n",
       " 561: '一致',\n",
       " 562: '手製',\n",
       " 563: '展示',\n",
       " 564: '著者',\n",
       " 565: '骨折り',\n",
       " 566: '遅延',\n",
       " 567: 'みろ',\n",
       " 568: '３０',\n",
       " 569: '再び',\n",
       " 570: '見捨て',\n",
       " 571: '逆',\n",
       " 572: '頼り',\n",
       " 573: 'たどりつ',\n",
       " 574: 'ぶどう',\n",
       " 575: '興奮',\n",
       " 576: '遅',\n",
       " 577: 'まん',\n",
       " 578: '名',\n",
       " 579: '渋滞',\n",
       " 580: 'ｎｔｔ',\n",
       " 581: '食器',\n",
       " 582: 'ほ',\n",
       " 583: '再開',\n",
       " 584: '定住',\n",
       " 585: 'と',\n",
       " 586: '鯨',\n",
       " 587: '概し',\n",
       " 588: 'いかが',\n",
       " 589: '選挙',\n",
       " 590: '紛れ',\n",
       " 591: 'ゆり',\n",
       " 592: '崖',\n",
       " 593: 'おもいだ',\n",
       " 594: '料理',\n",
       " 595: '最後',\n",
       " 596: 'うつぶせ',\n",
       " 597: 'この',\n",
       " 598: '明かり',\n",
       " 599: 'ざる',\n",
       " 600: 'ふもと',\n",
       " 601: 'ほっと',\n",
       " 602: '郵便',\n",
       " 603: 'コツ',\n",
       " 604: '持ち合わせ',\n",
       " 605: 'かっこつけ',\n",
       " 606: '戦',\n",
       " 607: '寝かせ',\n",
       " 608: '広',\n",
       " 609: '清め',\n",
       " 610: 'カード',\n",
       " 611: '調剤',\n",
       " 612: '宝石',\n",
       " 613: '誠',\n",
       " 614: '極端',\n",
       " 615: '濃霧',\n",
       " 616: '馳走',\n",
       " 617: '空路',\n",
       " 618: '切り抜け',\n",
       " 619: 'や',\n",
       " 620: 'おたふく',\n",
       " 621: '巣',\n",
       " 622: 'もう',\n",
       " 623: 'ざっと',\n",
       " 624: 'きゃ',\n",
       " 625: 'どたん',\n",
       " 626: '人間',\n",
       " 627: '流感',\n",
       " 628: '力添え',\n",
       " 629: '怒鳴りつけ',\n",
       " 630: 'おち',\n",
       " 631: '爆発',\n",
       " 632: '金回り',\n",
       " 633: '署',\n",
       " 634: '口論',\n",
       " 635: 'くび',\n",
       " 636: '立入り',\n",
       " 637: '一睡',\n",
       " 638: 'なにし',\n",
       " 639: 'みせ',\n",
       " 640: 'あぶれ',\n",
       " 641: '振る舞い',\n",
       " 642: '償い',\n",
       " 643: '公衆',\n",
       " 644: '来客',\n",
       " 645: '安らか',\n",
       " 646: '項目',\n",
       " 647: '企て',\n",
       " 648: '焦',\n",
       " 649: 'チョット',\n",
       " 650: 'もうけ',\n",
       " 651: '安全',\n",
       " 652: '歩き回',\n",
       " 653: '昼寝',\n",
       " 654: 'くま',\n",
       " 655: '引き出し',\n",
       " 656: '単調',\n",
       " 657: '入国',\n",
       " 658: 'あそこ',\n",
       " 659: 'テキサス',\n",
       " 660: 'エイズ',\n",
       " 661: 'せよ',\n",
       " 662: '特別',\n",
       " 663: 'いかい',\n",
       " 664: '真面目',\n",
       " 665: '便秘',\n",
       " 666: '折',\n",
       " 667: '電話',\n",
       " 668: '毎月',\n",
       " 669: 'さあお',\n",
       " 670: 'デザイナー',\n",
       " 671: '落第',\n",
       " 672: '楽',\n",
       " 673: '青筋',\n",
       " 674: 'かえ',\n",
       " 675: 'けが',\n",
       " 676: '試食',\n",
       " 677: 'パイ',\n",
       " 678: 'の',\n",
       " 679: '勿論',\n",
       " 680: '見ま',\n",
       " 681: 'ほぼ',\n",
       " 682: '帝王',\n",
       " 683: '最大',\n",
       " 684: 'さまざま',\n",
       " 685: '冷凍',\n",
       " 686: 'まさに',\n",
       " 687: 'やり遂げ',\n",
       " 688: '名医',\n",
       " 689: 'ハンサム',\n",
       " 690: '間待',\n",
       " 691: 'エッチ',\n",
       " 692: '兄',\n",
       " 693: '些細',\n",
       " 694: '感想',\n",
       " 695: '猛烈',\n",
       " 696: '証拠',\n",
       " 697: '解明',\n",
       " 698: 'きっかり',\n",
       " 699: '先生',\n",
       " 700: '銘記',\n",
       " 701: '多々',\n",
       " 702: '痙攣',\n",
       " 703: '芝居',\n",
       " 704: 'プロ',\n",
       " 705: '登れ',\n",
       " 706: 'もり',\n",
       " 707: '味方',\n",
       " 708: 'ぐれ',\n",
       " 709: '霧',\n",
       " 710: '和睦',\n",
       " 711: '１９８０',\n",
       " 712: 'きけ',\n",
       " 713: '失',\n",
       " 714: '抜群',\n",
       " 715: '必ず',\n",
       " 716: '半数',\n",
       " 717: '爪',\n",
       " 718: '値切',\n",
       " 719: '見当た',\n",
       " 720: '闇夜',\n",
       " 721: '手先',\n",
       " 722: 'デビッド',\n",
       " 723: '願え',\n",
       " 724: 'クラブ',\n",
       " 725: '更け',\n",
       " 726: '暮らし',\n",
       " 727: 'えぇ',\n",
       " 728: '立身',\n",
       " 729: '夕飯',\n",
       " 730: '落ちぶれ',\n",
       " 731: '高',\n",
       " 732: 'なっ',\n",
       " 733: '人付き合い',\n",
       " 734: 'スイス',\n",
       " 735: '熟',\n",
       " 736: 'そもそも',\n",
       " 737: '登記',\n",
       " 738: '外食',\n",
       " 739: 'いいあ',\n",
       " 740: '裏切',\n",
       " 741: 'うけたまわ',\n",
       " 742: '盲腸',\n",
       " 743: 'すみずみ',\n",
       " 744: 'られん',\n",
       " 745: 'かっ',\n",
       " 746: 'かわいらし',\n",
       " 747: 'リーダーシップ',\n",
       " 748: '待た',\n",
       " 749: '公立',\n",
       " 750: '恥じ',\n",
       " 751: 'ロッド',\n",
       " 752: '小道',\n",
       " 753: '叫',\n",
       " 754: 'のぼり',\n",
       " 755: 'しょ',\n",
       " 756: 'むずかし',\n",
       " 757: '数字',\n",
       " 758: 'グレー',\n",
       " 759: 'スチュワーデス',\n",
       " 760: '詳し',\n",
       " 761: '反響',\n",
       " 762: 'ヒット',\n",
       " 763: 'とうさん',\n",
       " 764: '集中',\n",
       " 765: '境遇',\n",
       " 766: '滞在',\n",
       " 767: '寝室',\n",
       " 768: '試着',\n",
       " 769: '現',\n",
       " 770: '以前',\n",
       " 771: '渡',\n",
       " 772: '遊学',\n",
       " 773: 'カッと',\n",
       " 774: '善悪',\n",
       " 775: '随分',\n",
       " 776: '扶養',\n",
       " 777: '不良',\n",
       " 778: '無頓着',\n",
       " 779: '付き添',\n",
       " 780: '地元',\n",
       " 781: '来',\n",
       " 782: 'チェンジ',\n",
       " 783: '多',\n",
       " 784: '変え',\n",
       " 785: '中家',\n",
       " 786: '高等',\n",
       " 787: '可決',\n",
       " 788: 'どきん',\n",
       " 789: 'はじめて',\n",
       " 790: '別物',\n",
       " 791: '抑え',\n",
       " 792: '姓',\n",
       " 793: 'きづまり',\n",
       " 794: 'てつだ',\n",
       " 795: 'しゃしん',\n",
       " 796: '人',\n",
       " 797: '近所',\n",
       " 798: 'やつ',\n",
       " 799: 'やわら',\n",
       " 800: '赤鉛筆',\n",
       " 801: '暗さ',\n",
       " 802: '翌週',\n",
       " 803: '間に合わせ',\n",
       " 804: 'ばかげ',\n",
       " 805: '僕',\n",
       " 806: '羨まし',\n",
       " 807: 'ブルック',\n",
       " 808: '開放',\n",
       " 809: 'ふけ',\n",
       " 810: '朝教会',\n",
       " 811: 'わざわざ',\n",
       " 812: '昼メシ',\n",
       " 813: 'どか',\n",
       " 814: '就業',\n",
       " 815: '出かけ',\n",
       " 816: '近頃',\n",
       " 817: '間静か',\n",
       " 818: 'とたん',\n",
       " 819: 'パトカー',\n",
       " 820: '口々',\n",
       " 821: '墓場',\n",
       " 822: 'おわ',\n",
       " 823: '惚れ込',\n",
       " 824: '唯一',\n",
       " 825: 'はたして',\n",
       " 826: 'メシ',\n",
       " 827: '浮かび上がれ',\n",
       " 828: '飛び込',\n",
       " 829: '食糧',\n",
       " 830: '始ま',\n",
       " 831: 'オーストラリア',\n",
       " 832: '帰り',\n",
       " 833: '横向き',\n",
       " 834: '寝つけ',\n",
       " 835: 'ボストン',\n",
       " 836: 'そそぎ',\n",
       " 837: '好調',\n",
       " 838: 'さし',\n",
       " 839: 'おう',\n",
       " 840: 'さよなら',\n",
       " 841: '漂流',\n",
       " 842: 'バス',\n",
       " 843: 'ふさい',\n",
       " 844: '法外',\n",
       " 845: '散',\n",
       " 846: '見落と',\n",
       " 847: '小説',\n",
       " 848: 'はたら',\n",
       " 849: '次回',\n",
       " 850: '休暇',\n",
       " 851: '閉じこも',\n",
       " 852: 'いやしくも',\n",
       " 853: '方便',\n",
       " 854: '帰省',\n",
       " 855: 'ら',\n",
       " 856: 'どっち',\n",
       " 857: '騙',\n",
       " 858: '放',\n",
       " 859: '暮し',\n",
       " 860: '！',\n",
       " 861: '寮',\n",
       " 862: '有料',\n",
       " 863: '企業',\n",
       " 864: 'ヘトヘト',\n",
       " 865: '破水',\n",
       " 866: '言外',\n",
       " 867: '劇場',\n",
       " 868: 'たが',\n",
       " 869: '漫画',\n",
       " 870: '難し',\n",
       " 871: '眼',\n",
       " 872: '退け',\n",
       " 873: 'そちら',\n",
       " 874: '禁固',\n",
       " 875: '本読',\n",
       " 876: '何処',\n",
       " 877: 'テニスチーム',\n",
       " 878: '自活',\n",
       " 879: '一躍',\n",
       " 880: 'グラマー',\n",
       " 881: '信号',\n",
       " 882: '精算',\n",
       " 883: '１２',\n",
       " 884: '提供',\n",
       " 885: '開業',\n",
       " 886: '便',\n",
       " 887: 'ニコル',\n",
       " 888: '日増し',\n",
       " 889: '０００',\n",
       " 890: '安物',\n",
       " 891: 'そうそう',\n",
       " 892: 'ｌｉｌｙ',\n",
       " 893: 'がまん',\n",
       " 894: '裂け',\n",
       " 895: 'だるま',\n",
       " 896: '階段',\n",
       " 897: '企',\n",
       " 898: 'カーター',\n",
       " 899: '背広',\n",
       " 900: '仲良し',\n",
       " 901: '風呂',\n",
       " 902: '様子',\n",
       " 903: '教訓',\n",
       " 904: '勝者',\n",
       " 905: '慣習',\n",
       " 906: '土壇場',\n",
       " 907: '陳列',\n",
       " 908: 'ずいぶん',\n",
       " 909: 'やめろ',\n",
       " 910: '陥',\n",
       " 911: '説得',\n",
       " 912: '名指し',\n",
       " 913: '前金',\n",
       " 914: '関わり合い',\n",
       " 915: '盛り',\n",
       " 916: '負か',\n",
       " 917: '名所見物',\n",
       " 918: '登校',\n",
       " 919: 'はげまし',\n",
       " 920: 'たい',\n",
       " 921: '鳥取',\n",
       " 922: '同級',\n",
       " 923: 'あらさがし',\n",
       " 924: '審議',\n",
       " 925: '承',\n",
       " 926: '救急',\n",
       " 927: '改め',\n",
       " 928: '悪酔い',\n",
       " 929: '停ま',\n",
       " 930: '冒',\n",
       " 931: '赤ん坊',\n",
       " 932: '寝返り',\n",
       " 933: '意図',\n",
       " 934: '軽蔑',\n",
       " 935: '渡れ',\n",
       " 936: '太刀打ち',\n",
       " 937: 'ごまか',\n",
       " 938: '代',\n",
       " 939: 'ベル',\n",
       " 940: '王',\n",
       " 941: 'ｓｏｓ',\n",
       " 942: '空っぽ',\n",
       " 943: 'ぜい',\n",
       " 944: '窓越し',\n",
       " 945: '折り返',\n",
       " 946: 'ほど',\n",
       " 947: 'メイ',\n",
       " 948: 'ブラシ',\n",
       " 949: '日曜日',\n",
       " 950: '泉',\n",
       " 951: '話しかけ',\n",
       " 952: 'まえ',\n",
       " 953: 'けさ',\n",
       " 954: '衰弱',\n",
       " 955: '熟練',\n",
       " 956: 'なんか',\n",
       " 957: '巡査',\n",
       " 958: '年配',\n",
       " 959: '抜歯',\n",
       " 960: 'あがめたて',\n",
       " 961: 'こうと',\n",
       " 962: 'ジミー',\n",
       " 963: '電気',\n",
       " 964: '痛み止め',\n",
       " 965: 'たく',\n",
       " 966: '傷口',\n",
       " 967: 'パット',\n",
       " 968: 'のし',\n",
       " 969: 'ひとこと',\n",
       " 970: 'かけ',\n",
       " 971: 'めと',\n",
       " 972: 'ファン',\n",
       " 973: '控え',\n",
       " 974: 'すが',\n",
       " 975: '手上げ',\n",
       " 976: '助手',\n",
       " 977: 'チャン',\n",
       " 978: '久し',\n",
       " 979: '偽り',\n",
       " 980: 'カゴ',\n",
       " 981: '重病',\n",
       " 982: '入選',\n",
       " 983: '盲目',\n",
       " 984: '壊れ',\n",
       " 985: '今着',\n",
       " 986: '親元',\n",
       " 987: 'ゴルフ',\n",
       " 988: '就',\n",
       " 989: 'キー',\n",
       " 990: 'ペン',\n",
       " 991: '薬',\n",
       " 992: 'ちか',\n",
       " 993: '昨晩',\n",
       " 994: 'こま',\n",
       " 995: '用心',\n",
       " 996: '触れ合',\n",
       " 997: 'たいくつ',\n",
       " 998: 'ページ',\n",
       " 999: '乗りこ',\n",
       " 1000: '果て',\n",
       " 1001: 'かそこ',\n",
       " 1002: 'さぼ',\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_i2w\n",
    "#e_i2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 各層クラスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のクラスの中では, 系列中の全てのステップに対してまとめて処理をおこなう`f_prop`関数の他に, 1つのステップに対してのみ処理をおこなう`f_prop_test`関数を実装しています. 理由は後述します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. 単語のEmbedding層\n",
    "\n",
    "$m$ : emb_dim\n",
    "\n",
    "$n$ : vocab_size\n",
    "\n",
    "実際にEmbedding層の処理を担うembedding_lookupでは, 入力をone_hotベクトルに変換し, Embedding層の行列に掛け, 対応する列ベクトルを選択します.\n",
    "<img src='img/embedding.png' width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale=0.08):\n",
    "        #self.V = tf.Variable(rng.randn(vocab_size, emb_dim).astype('float32') * scale, name='V')\n",
    "        self.V = tf.Variable(tf.random_uniform([vocab_size, emb_dim], -1.0, 1.0),name=\"V\")\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        return tf.nn.embedding_lookup(self.V, x_t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Long short-term memory (LSTM)\n",
    "<img src=\"https://tc.sinaimg.cn/maxwidth.800/tc.service.weibo.com/cdn_images_1_medium_com/a1ced418f9a4f71b2d32fc28f7242f03.jpg\">\n",
    "<img src=\"https://tc.sinaimg.cn/maxwidth.800/tc.service.weibo.com/cdn_images_1_medium_com/8bbbd006ba6398d3bb77444e7df6be26.png\">\n",
    "<img src=\"https://tc.sinaimg.cn/maxwidth.800/tc.service.weibo.com/cdn_images_1_medium_com/58ad765e09eacb5116c9dfc5897c7296.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装する式は次のようになります. ($\\odot$は要素ごとの積)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 入力ゲート: $\\hspace{20mm}i_t = \\sigma \\left( W_{xi} x_t + W_{hi} h_{t-1} + b_i \\right)$\n",
    "- 忘却ゲート: $\\hspace{20mm}f_t = \\sigma \\left( W_{xf} x_t + W_{hf} h_{t-1} + b_f \\right)$  \n",
    "- 出力ゲート: $\\hspace{20mm}o_t = \\sigma \\left( W_{xo} x_t + W_{ho} h_{t-1} + b_o \\right)$  \n",
    "- セル:　　　 $\\hspace{20mm}c_t = f_t \\odot c_{t-1} + i_t \\odot \\tanh \\left( W_{xc} x_t + W_{hc} h_{t-1} + b_c \\right)$  \n",
    "- 隠れ層: 　　$\\hspace{20mm}h_t = o_t \\odot \\tanh \\left( c_t \\right)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 入力ゲート: 入力と隠れ層を使って0~1に変換　->　入力と隠れ層を使って非線形変換したものをどれだけ使うか・・①\n",
    "- 忘却ゲート: 入力と隠れ層を使って0~1に変換　->　 一つ前のセル状態をどれくらい使うか・・②\n",
    "- 出力ゲート: 入力と隠れ層を使って0~1に変換　->　非線形変換したセル状態をどれくらい使うか・・③\n",
    "\n",
    "\n",
    "- セル:　②+①\n",
    "- 隠れ層: 　　③  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純なRNNでは各ステップの関数の戻り値は隠れ層のみ ($h_t$) でしたが, LSTMでは隠れ層とセル状態の2つ ($h_t, c_t$) となるので注意してください. またマスクに関しても両方に適用する必要があります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, in_dim, hid_dim, m, h_0=None, c_0=None):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        # input gate\n",
    "        self.W_xi = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xi')\n",
    "        self.W_hi = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hi')\n",
    "        self.b_i  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_i')\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_xf = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.W_hf = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.b_f  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_f')\n",
    "\n",
    "        # output gate\n",
    "        self.W_xo = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xo')\n",
    "        self.W_ho = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_ho')\n",
    "        self.b_o  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_o')\n",
    "\n",
    "        # cell state\n",
    "        self.W_xc = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xc')\n",
    "        self.W_hc = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hc')\n",
    "        self.b_c  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_c')\n",
    "\n",
    "        # initial state\n",
    "        self.h_0 = h_0\n",
    "        self.c_0 = c_0\n",
    "\n",
    "        # mask\n",
    "        self.m = m\n",
    "    \"\"\"（参考）RNNでの実装\n",
    "    def f_prop(self, x):\n",
    "        def fn(h_tm1, x_and_m):\n",
    "            x = x_and_m[0]\n",
    "            m = x_and_m[1]\n",
    "            h_t = tf.nn.tanh(tf.matmul(h_tm1, self.W_re) + tf.matmul(x, self.W_in) + self.b_re)\n",
    "            return m[:, None] * h_t + (1 - m[:, None]) * h_tm1 # Mask\n",
    "\n",
    "        # shape: [batch_size, sentence_length, in_dim] -> shape: [sentence_length, batch_size, in_dim]\n",
    "        _x = tf.transpose(x, perm=[1, 0, 2])\n",
    "        # shape: [batch_size, sentence_length] -> shape: [sentence_length, batch_size]\n",
    "        _m = tf.transpose(self.m)\n",
    "        h_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim])) # Initial state\n",
    "        \n",
    "        h = tf.scan(fn=fn, elems=[_x, _m], initializer=h_0)\n",
    "        \n",
    "        return h[-1] # Take the last state\n",
    "    \"\"\"\n",
    "    def f_prop(self, x):\n",
    "        def fn(tm1, x_and_m):\n",
    "            h_tm1 = tm1[0]      #hidden state\n",
    "            c_tm1 = tm1[1]      #cell state\n",
    "            x_t = x_and_m[0]   #入力\n",
    "            m_t = x_and_m[1]  #マスク\n",
    "            # input gate\n",
    "            i_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xi) + tf.matmul(h_tm1, self.W_hi) + self.b_i)\n",
    "\n",
    "            # forget gate\n",
    "            f_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xf) + tf.matmul(h_tm1, self.W_hf) + self.b_f)\n",
    "\n",
    "            # output gate\n",
    "            o_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xo) + tf.matmul(h_tm1, self.W_ho) + self.b_o)\n",
    "\n",
    "            # cell state  forget*前のcell + input* （入力×重み＋hidden*重み）\n",
    "            c_t = f_t * c_tm1 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(h_tm1, self.W_hc) + self.b_c)\n",
    "            c_t = m_t[:, np.newaxis] * c_t + (1. - m_t[:, np.newaxis]) * c_tm1 # Mask\n",
    "\n",
    "            # hidden state　output*tanh(今cell)\n",
    "            h_t = o_t * tf.nn.tanh(c_t)\n",
    "            h_t = m_t[:, np.newaxis] * h_t + (1. - m_t[:, np.newaxis]) * h_tm1 # Mask\n",
    "\n",
    "            return [h_t, c_t]\n",
    "\n",
    "        _x = tf.transpose(x, perm=[1, 0, 2])\n",
    "        _m = tf.transpose(self.m)\n",
    "\n",
    "        if self.h_0 == None:\n",
    "            self.h_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "        if self.c_0 == None:\n",
    "            self.c_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "\n",
    "        h, c = tf.scan(fn=fn, elems=[_x, _m], initializer=[self.h_0, self.c_0])\n",
    "        return tf.transpose(h, perm=[1, 0, 2]), tf.transpose(c, perm=[1, 0, 2])\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        # input gate\n",
    "        i_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xi) + tf.matmul(self.h_0, self.W_hi) + self.b_i)\n",
    "\n",
    "        # forget gate\n",
    "        f_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xf) + tf.matmul(self.h_0, self.W_hf) + self.b_f)\n",
    "\n",
    "        # output gate\n",
    "        o_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_xo) + tf.matmul(self.h_0, self.W_ho) + self.b_o)\n",
    "\n",
    "        # cell state\n",
    "        c_t = f_t * self.c_0 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(self.h_0, self.W_hc) + self.b_c)\n",
    "\n",
    "        # hidden state\n",
    "        h_t = o_t * tf.nn.tanh(c_t)\n",
    "\n",
    "        return [h_t, c_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. 全結合層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f_prop`における入力は3階テンソルとなるので, `tf.einsum`を使用します.\n",
    "\n",
    "入力`x`と重み`W`のshapeは, それぞれ\n",
    "\n",
    "- `x`: (ミニバッチサイズ, i) x (系列長, j) x (入力次元数, k)\n",
    "- `W`: (入力次元数, k) x (出力次元数, l)\n",
    "\n",
    "で, 出力は\n",
    "\n",
    "- (ミニバッチサイズ, i) x (系列長, j) x (出力次元数, l)\n",
    "\n",
    "となるので, `einsum`の第一引数の表記は`'ijk,kl->ijl'`となります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        # Xavier\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                        low=-np.sqrt(6/(in_dim + out_dim)),\n",
    "                        high=np.sqrt(6/(in_dim + out_dim)),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32'), name='W')\n",
    "        self.b = tf.Variable(tf.zeros([out_dim], dtype=tf.float32), name='b')\n",
    "        self.function = function\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return self.function(tf.einsum('ijk,kl->ijl', x, self.W) + self.b)\n",
    "\n",
    "    def f_prop_test(self, x_t):\n",
    "        return self.function(tf.matmul(x_t, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 計算グラフ構築 & パラメータの更新設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下の図のモデルを実装します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/seq2seq.png' width=600>\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/tutorials/seq2seq より引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ミニバッチサイズ, 系列長, 辞書のサイズをそれぞれ$N$, $T$, $K$とすると, 多クラス交差エントロピー誤差関数は次のようになります.\n",
    "\n",
    "$$\n",
    "    E({\\bf \\theta}) = -\\frac{1}{N}\\sum^N_{n=1}\\sum^T_{t=1}\\sum^K_{k=1} d^{(n)}_{t, k} \\log y^{(n)}_{t, k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder, Decoderともに短い系列に対してはpaddingをします. また, Encoderではマスクを行います.\n",
    "\n",
    "Decoderでpaddingした部分については, コストがゼロになるようにします. これは, 単語がある部分を1, paddingの部分を0とするバイナリのマスクをかけるか, paddingの部分の教師ラベルdの要素をすべてゼロになるようにします.\n",
    "\n",
    "`tf`においては, `tf.one_hot`でone_hot化するときに範囲外の値(-1など)を入力とすれば, その値に対するベクトルはすべてゼロとなります. 以下ではこちらの方法で実装しています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hang': 3,\n",
       " 'seldom': 4,\n",
       " 'octopus': 5,\n",
       " 'and': 6,\n",
       " 'hiccup': 7,\n",
       " 'fatigued': 8,\n",
       " 'follow': 9,\n",
       " 'science-fiction': 10,\n",
       " 'prince': 11,\n",
       " 'opinions': 12,\n",
       " 'accompanies': 13,\n",
       " 'named': 14,\n",
       " 'stain': 15,\n",
       " 'whom': 16,\n",
       " 'joke': 17,\n",
       " 'described': 18,\n",
       " 'nicely': 19,\n",
       " 'loose': 20,\n",
       " 'mayumi': 21,\n",
       " 'pharmacy': 22,\n",
       " 'sultry': 23,\n",
       " 'arrest': 24,\n",
       " 'arrival': 25,\n",
       " 'death': 26,\n",
       " 'canteen': 27,\n",
       " 'wrestlers': 28,\n",
       " 'papers': 29,\n",
       " 'accept': 30,\n",
       " 'effect': 31,\n",
       " 'obtain': 32,\n",
       " 'watch': 33,\n",
       " 'grown': 34,\n",
       " 'cats': 35,\n",
       " 'accompanied': 36,\n",
       " 'graded': 37,\n",
       " 'left': 38,\n",
       " 'struck': 39,\n",
       " 'listened': 40,\n",
       " 'benefit': 41,\n",
       " 'foxed': 42,\n",
       " 'open': 43,\n",
       " 'taste': 44,\n",
       " 'frozen': 45,\n",
       " 'hesitates': 46,\n",
       " 'vowels': 47,\n",
       " 'twenties': 48,\n",
       " 'talents': 49,\n",
       " 'towers': 50,\n",
       " 'competent': 51,\n",
       " 'frantic': 52,\n",
       " 'learning': 53,\n",
       " 'sugar': 54,\n",
       " 'low': 55,\n",
       " 'good-bye': 56,\n",
       " 'leads': 57,\n",
       " 'flung': 58,\n",
       " 'calls': 59,\n",
       " 'tumbled': 60,\n",
       " 'flouted': 61,\n",
       " 'punctual': 62,\n",
       " 'exhibition': 63,\n",
       " 'chicken': 64,\n",
       " 'degrees': 65,\n",
       " 'copy': 66,\n",
       " 'height': 67,\n",
       " 'apart': 68,\n",
       " 'tour': 69,\n",
       " 'forty': 70,\n",
       " 'exacted': 71,\n",
       " 'funniest': 72,\n",
       " 'participated': 73,\n",
       " 'pans': 74,\n",
       " 'lesson': 75,\n",
       " 'nuclear': 76,\n",
       " 'dutch': 77,\n",
       " 'fact': 78,\n",
       " 'inconceivable': 79,\n",
       " 'truthful': 80,\n",
       " 'hearing': 81,\n",
       " 'ventilation': 82,\n",
       " 'net': 83,\n",
       " 'check': 84,\n",
       " 'expects': 85,\n",
       " 'thompson': 86,\n",
       " 'beloved': 87,\n",
       " 'affords': 88,\n",
       " 'slumbered': 89,\n",
       " 'forsake': 90,\n",
       " 'artificial': 91,\n",
       " 'thirty-one': 92,\n",
       " 'delight': 93,\n",
       " 'waiter': 94,\n",
       " 'appearance': 95,\n",
       " 'madman': 96,\n",
       " 'rests': 97,\n",
       " 'round': 98,\n",
       " 'endeavored': 99,\n",
       " 'teachers': 100,\n",
       " 'wintertime': 101,\n",
       " 'understands': 102,\n",
       " 'buildings': 103,\n",
       " 'farewell': 104,\n",
       " 'air-conditioned': 105,\n",
       " 'greet': 106,\n",
       " 'drives': 107,\n",
       " 'ringing': 108,\n",
       " 'sells': 109,\n",
       " 'unusual': 110,\n",
       " 'paid': 111,\n",
       " 'ever': 112,\n",
       " '3-2': 113,\n",
       " 'window': 114,\n",
       " 'matters': 115,\n",
       " 'sake': 116,\n",
       " 'crowded': 117,\n",
       " 'practiced': 118,\n",
       " 'ok': 119,\n",
       " 'owner': 120,\n",
       " 'weary': 121,\n",
       " 'party': 122,\n",
       " 'human': 123,\n",
       " 'slide': 124,\n",
       " 'urban': 125,\n",
       " 'build': 126,\n",
       " 'chicago': 127,\n",
       " 'accurately': 128,\n",
       " 'pork': 129,\n",
       " 'objection': 130,\n",
       " 'repair': 131,\n",
       " 'dates': 132,\n",
       " 'contingent': 133,\n",
       " 'officer': 134,\n",
       " 'brightened': 135,\n",
       " 'overworked': 136,\n",
       " 'upset': 137,\n",
       " 'hat': 138,\n",
       " 'ray': 139,\n",
       " 'hi': 140,\n",
       " 'elementary': 141,\n",
       " 'drowsy': 142,\n",
       " 'foundation': 143,\n",
       " 'cook': 144,\n",
       " 'justice': 145,\n",
       " 'sent': 146,\n",
       " 'handy': 147,\n",
       " 'nagoya': 148,\n",
       " 'sore': 149,\n",
       " 'east': 150,\n",
       " 'jack': 151,\n",
       " 'blamed': 152,\n",
       " 'cds': 153,\n",
       " 'fewer': 154,\n",
       " 'continued': 155,\n",
       " 'thumbs': 156,\n",
       " 'nearing': 157,\n",
       " 'walls': 158,\n",
       " 'doubtless': 159,\n",
       " 'gained': 160,\n",
       " 'populous': 161,\n",
       " 'stomachache': 162,\n",
       " 'empty': 163,\n",
       " 'reply': 164,\n",
       " 'dick': 165,\n",
       " 'sings': 166,\n",
       " 'injures': 167,\n",
       " 'regard': 168,\n",
       " 'comfort': 169,\n",
       " 'cost': 170,\n",
       " 'picky': 171,\n",
       " 'eventually': 172,\n",
       " 'below': 173,\n",
       " 'section': 174,\n",
       " 'pigeon-toed': 175,\n",
       " 'entreat': 176,\n",
       " 'enjoy': 177,\n",
       " 'neighborhood': 178,\n",
       " 'credit': 179,\n",
       " 'deposited': 180,\n",
       " 'acquaintance': 181,\n",
       " 'replaced': 182,\n",
       " 'henry': 183,\n",
       " 'some': 184,\n",
       " 'abc': 185,\n",
       " 'merely': 186,\n",
       " 'auntie': 187,\n",
       " 'abhorrent': 188,\n",
       " 'him': 189,\n",
       " 'brush': 190,\n",
       " 'seasoning': 191,\n",
       " \"'s\": 192,\n",
       " 'engine': 193,\n",
       " 'recovered': 194,\n",
       " 'extra': 195,\n",
       " 'flowers': 196,\n",
       " 'supports': 197,\n",
       " 'share': 198,\n",
       " 'chilly': 199,\n",
       " 'infected': 200,\n",
       " 'abroad': 201,\n",
       " 'neighbors': 202,\n",
       " 'falls': 203,\n",
       " 'fighter': 204,\n",
       " 'pushover': 205,\n",
       " 'brothers': 206,\n",
       " 'reimbursed': 207,\n",
       " 'fur': 208,\n",
       " 'telling': 209,\n",
       " 'otherwise': 210,\n",
       " 'compared': 211,\n",
       " 'room': 212,\n",
       " 'switzerland': 213,\n",
       " 'in': 214,\n",
       " 'keys': 215,\n",
       " 'surface': 216,\n",
       " 'against': 217,\n",
       " 'desires': 218,\n",
       " 'adult': 219,\n",
       " 'noted': 220,\n",
       " 'hometown': 221,\n",
       " 'arranged': 222,\n",
       " 'moody': 223,\n",
       " 'gate': 224,\n",
       " 'discussing': 225,\n",
       " 'borrowed': 226,\n",
       " 'treat': 227,\n",
       " ',': 228,\n",
       " 'beard': 229,\n",
       " 'dieting': 230,\n",
       " '18': 231,\n",
       " 'funds': 232,\n",
       " 'merciful': 233,\n",
       " 'indoors': 234,\n",
       " 'length': 235,\n",
       " 'steve': 236,\n",
       " 'substitute': 237,\n",
       " 'persevere': 238,\n",
       " 'obverse': 239,\n",
       " 'nephew': 240,\n",
       " 'even': 241,\n",
       " 'agreement': 242,\n",
       " 'citizens': 243,\n",
       " 'shorthanded': 244,\n",
       " 'copies': 245,\n",
       " 'house': 246,\n",
       " 'farm': 247,\n",
       " 'complaint': 248,\n",
       " 'chris': 249,\n",
       " 'fax': 250,\n",
       " 'tale': 251,\n",
       " 'crouch': 252,\n",
       " 'antithesis': 253,\n",
       " 'yo': 254,\n",
       " 'conflict': 255,\n",
       " 'consequence': 256,\n",
       " 'snow-capped': 257,\n",
       " 'camping': 258,\n",
       " 'unsuitable': 259,\n",
       " 'cab': 260,\n",
       " 'stillborn': 261,\n",
       " 'beating': 262,\n",
       " 'absolutely': 263,\n",
       " 'kicking': 264,\n",
       " 'stressed': 265,\n",
       " 'ought': 266,\n",
       " 'impressive': 267,\n",
       " 'praise': 268,\n",
       " 'galore': 269,\n",
       " 'holds': 270,\n",
       " 'buyer': 271,\n",
       " 'fuck-buddies': 272,\n",
       " 'information': 273,\n",
       " 'suzuki': 274,\n",
       " 'lamented': 275,\n",
       " 'screamed': 276,\n",
       " 'resort': 277,\n",
       " 'jacket': 278,\n",
       " 'door': 279,\n",
       " 'employ': 280,\n",
       " 'water': 281,\n",
       " 'amsterdam': 282,\n",
       " 'forgetting': 283,\n",
       " 'hail': 284,\n",
       " 'kindness': 285,\n",
       " 'scene': 286,\n",
       " 'moves': 287,\n",
       " 'axis': 288,\n",
       " 'alarm': 289,\n",
       " 'robbers': 290,\n",
       " 'great': 291,\n",
       " 'suffered': 292,\n",
       " 'incidents': 293,\n",
       " 'shop': 294,\n",
       " 'scientific': 295,\n",
       " 'internet': 296,\n",
       " 'old': 297,\n",
       " 'entrepreneur': 298,\n",
       " 'okayama': 299,\n",
       " 'fanned': 300,\n",
       " 'undertaken': 301,\n",
       " 'bullet': 302,\n",
       " 'or': 303,\n",
       " 'wears': 304,\n",
       " 'bedroom': 305,\n",
       " 'pets': 306,\n",
       " 'speculating': 307,\n",
       " 'highest': 308,\n",
       " 'standing': 309,\n",
       " 'determination': 310,\n",
       " 'translated': 311,\n",
       " 'prefer': 312,\n",
       " 'needed': 313,\n",
       " 'trust': 314,\n",
       " '10': 315,\n",
       " 'dave': 316,\n",
       " 'dared': 317,\n",
       " 'afterward': 318,\n",
       " 'dispute': 319,\n",
       " 'gods': 320,\n",
       " 'headlights': 321,\n",
       " 'preface': 322,\n",
       " 'clouds': 323,\n",
       " 'liberty': 324,\n",
       " 'toward': 325,\n",
       " 'sure': 326,\n",
       " 'earthquakes': 327,\n",
       " 'resumed': 328,\n",
       " 'obvious': 329,\n",
       " 'lawyer': 330,\n",
       " 'shy': 331,\n",
       " 'widely': 332,\n",
       " 'happens': 333,\n",
       " 'right': 334,\n",
       " 'vegetables': 335,\n",
       " 'accomplish': 336,\n",
       " 'nile': 337,\n",
       " 'sink': 338,\n",
       " 'huge': 339,\n",
       " 'bawling': 340,\n",
       " 'quieter': 341,\n",
       " 'mailbox': 342,\n",
       " 'goodness': 343,\n",
       " 'painted': 344,\n",
       " 'dissatisfied': 345,\n",
       " 'thanksgiving': 346,\n",
       " 'board': 347,\n",
       " 'deliver': 348,\n",
       " 'dogs': 349,\n",
       " 'cold-blooded': 350,\n",
       " 'spelled': 351,\n",
       " 'wallet': 352,\n",
       " 'owed': 353,\n",
       " 'physicist': 354,\n",
       " 'seeking': 355,\n",
       " 'sit': 356,\n",
       " 'programmer': 357,\n",
       " 'lasted': 358,\n",
       " 'candles': 359,\n",
       " 'depressed': 360,\n",
       " 'notebooks': 361,\n",
       " 'too': 362,\n",
       " 'eludes': 363,\n",
       " 'sundays': 364,\n",
       " 'living': 365,\n",
       " 'sheep': 366,\n",
       " 'air': 367,\n",
       " 'smoker': 368,\n",
       " '150': 369,\n",
       " 'ears': 370,\n",
       " 'recognize': 371,\n",
       " 'absurdly': 372,\n",
       " 'grimaced': 373,\n",
       " 'gestures': 374,\n",
       " 'two': 375,\n",
       " 'reality': 376,\n",
       " 'official': 377,\n",
       " 'lead': 378,\n",
       " 'rope': 379,\n",
       " 'britain': 380,\n",
       " 'pupil': 381,\n",
       " 'wrong': 382,\n",
       " 'director': 383,\n",
       " 'rules': 384,\n",
       " 'entertained': 385,\n",
       " 'vending': 386,\n",
       " 'agitated': 387,\n",
       " 'rags': 388,\n",
       " 'records': 389,\n",
       " 'came': 390,\n",
       " 'apples': 391,\n",
       " 'wax': 392,\n",
       " 'industrious': 393,\n",
       " 'wishes': 394,\n",
       " 'kushiro': 395,\n",
       " 'typical': 396,\n",
       " 'wages': 397,\n",
       " 'easy': 398,\n",
       " 'anyone': 399,\n",
       " 'scores': 400,\n",
       " 'wanna': 401,\n",
       " 'shook': 402,\n",
       " 'impatient': 403,\n",
       " 'ideas': 404,\n",
       " 'revelation': 405,\n",
       " 'english': 406,\n",
       " 'arrange': 407,\n",
       " 'bugs': 408,\n",
       " 'green': 409,\n",
       " 'londoner': 410,\n",
       " 'jam': 411,\n",
       " 'england': 412,\n",
       " 'disguised': 413,\n",
       " 'swam': 414,\n",
       " 'favour': 415,\n",
       " 'western': 416,\n",
       " 'indisposed': 417,\n",
       " 'sucker': 418,\n",
       " 'cooking': 419,\n",
       " 'bottom': 420,\n",
       " 'retire': 421,\n",
       " 'powers': 422,\n",
       " 'subtract': 423,\n",
       " 'films': 424,\n",
       " 'mild': 425,\n",
       " 'directed': 426,\n",
       " 'law': 427,\n",
       " 'stool': 428,\n",
       " 'anxieties': 429,\n",
       " 'agreeable': 430,\n",
       " 'japan': 431,\n",
       " 'offered': 432,\n",
       " 'complain': 433,\n",
       " 'ascribed': 434,\n",
       " 'judy': 435,\n",
       " 'trivial': 436,\n",
       " 'mechanic': 437,\n",
       " 'justly': 438,\n",
       " 'backed': 439,\n",
       " 'jones': 440,\n",
       " 'suppose': 441,\n",
       " 'violent': 442,\n",
       " 'weekdays': 443,\n",
       " 'carelessly': 444,\n",
       " 'shampoo': 445,\n",
       " 'correspond': 446,\n",
       " 'operational': 447,\n",
       " 'friendless': 448,\n",
       " 'flew': 449,\n",
       " 'forced': 450,\n",
       " 'doubted': 451,\n",
       " 'parallel': 452,\n",
       " 'leaned': 453,\n",
       " 'passport': 454,\n",
       " 'proposition': 455,\n",
       " 'just': 456,\n",
       " 'shuttle': 457,\n",
       " 'told': 458,\n",
       " 'grow': 459,\n",
       " 'nimble': 460,\n",
       " 'shrank': 461,\n",
       " 'aware': 462,\n",
       " 'chilled': 463,\n",
       " 'birth': 464,\n",
       " 'estella': 465,\n",
       " 'bottle': 466,\n",
       " 'gradually': 467,\n",
       " 'saw': 468,\n",
       " 'pays': 469,\n",
       " 'gold': 470,\n",
       " 'excluded': 471,\n",
       " 'quarter': 472,\n",
       " 'blow': 473,\n",
       " 'carefree': 474,\n",
       " 'lightly': 475,\n",
       " 'rejoiced': 476,\n",
       " 'rude': 477,\n",
       " 'videotape': 478,\n",
       " 'missing': 479,\n",
       " 'tremble': 480,\n",
       " 'polluted': 481,\n",
       " 'rudeness': 482,\n",
       " 'progressed': 483,\n",
       " 'bees': 484,\n",
       " 'defended': 485,\n",
       " 'base': 486,\n",
       " 'motorcycle': 487,\n",
       " 'revealed': 488,\n",
       " 'yet': 489,\n",
       " 'performance': 490,\n",
       " 'recovery': 491,\n",
       " 'remodeled': 492,\n",
       " 'mud': 493,\n",
       " 'helping': 494,\n",
       " 'tell': 495,\n",
       " 'constructive': 496,\n",
       " 'complex': 497,\n",
       " 'die': 498,\n",
       " 'speak': 499,\n",
       " 'crazy': 500,\n",
       " 'aside': 501,\n",
       " 'painter': 502,\n",
       " 'tuesday': 503,\n",
       " 'joking': 504,\n",
       " 'negative': 505,\n",
       " 'are': 506,\n",
       " 'parted': 507,\n",
       " 'weeks': 508,\n",
       " 'melt': 509,\n",
       " 'akiko': 510,\n",
       " 'eighteen': 511,\n",
       " 'doing': 512,\n",
       " \"'ve\": 513,\n",
       " 'opportune': 514,\n",
       " 'arriving': 515,\n",
       " 'attacked': 516,\n",
       " 'where': 517,\n",
       " 'pressed': 518,\n",
       " 'gonna': 519,\n",
       " 'ferried': 520,\n",
       " 'hangover': 521,\n",
       " 'shouted': 522,\n",
       " 'jerked': 523,\n",
       " 'doubt': 524,\n",
       " 'adjourned': 525,\n",
       " 'depending': 526,\n",
       " 'sealed': 527,\n",
       " 'owing': 528,\n",
       " 'ready': 529,\n",
       " 'regularly': 530,\n",
       " 'desks': 531,\n",
       " 'ensure': 532,\n",
       " 'catches': 533,\n",
       " 'betray': 534,\n",
       " 'basks': 535,\n",
       " 'eyesore': 536,\n",
       " 'audition': 537,\n",
       " 'trips': 538,\n",
       " 'prettiest': 539,\n",
       " 'shares': 540,\n",
       " 'appreciated': 541,\n",
       " 'worse': 542,\n",
       " 'freeway': 543,\n",
       " 'flies': 544,\n",
       " 'invader': 545,\n",
       " 'marvels': 546,\n",
       " 'adults': 547,\n",
       " 'vice': 548,\n",
       " 'anger': 549,\n",
       " 'freezing': 550,\n",
       " 'method': 551,\n",
       " 'signal': 552,\n",
       " 'college': 553,\n",
       " 'ground': 554,\n",
       " 'utterly': 555,\n",
       " 'knew': 556,\n",
       " 'effort': 557,\n",
       " 'corresponding': 558,\n",
       " 'exposures': 559,\n",
       " 'introduction': 560,\n",
       " 'yamanaka': 561,\n",
       " 'called': 562,\n",
       " 'awfully': 563,\n",
       " 'sum': 564,\n",
       " 'snitch': 565,\n",
       " 'dear': 566,\n",
       " 'farce': 567,\n",
       " 'puppy': 568,\n",
       " 'deep': 569,\n",
       " 'treatment': 570,\n",
       " 'successfully': 571,\n",
       " 'candidates': 572,\n",
       " 'lion': 573,\n",
       " 'bad': 574,\n",
       " 'seaweed': 575,\n",
       " 'remarked': 576,\n",
       " 'paper': 577,\n",
       " 'dilemma': 578,\n",
       " 'live': 579,\n",
       " 'produce': 580,\n",
       " 'pearl': 581,\n",
       " 'blunders': 582,\n",
       " 'indicate': 583,\n",
       " 'inconvenienced': 584,\n",
       " 'fetch': 585,\n",
       " 'concern': 586,\n",
       " 'moon': 587,\n",
       " 'salad': 588,\n",
       " 'hokkaido': 589,\n",
       " 'observe': 590,\n",
       " 'planning': 591,\n",
       " 'nothing': 592,\n",
       " 'masao': 593,\n",
       " 'seaside': 594,\n",
       " 'moderation': 595,\n",
       " 'press': 596,\n",
       " 'finding': 597,\n",
       " 'moaning': 598,\n",
       " 'okay': 599,\n",
       " 'exchanged': 600,\n",
       " 'fan': 601,\n",
       " 'woman': 602,\n",
       " 'dozed': 603,\n",
       " 'center': 604,\n",
       " 'insurance': 605,\n",
       " 'distances': 606,\n",
       " 'ache': 607,\n",
       " 'endure': 608,\n",
       " 'five-year-old': 609,\n",
       " 'shines': 610,\n",
       " 'which': 611,\n",
       " 'scars': 612,\n",
       " 'escapes': 613,\n",
       " 'compliment': 614,\n",
       " 'exist': 615,\n",
       " 'spoon': 616,\n",
       " 'out': 617,\n",
       " 'manager': 618,\n",
       " 'trick': 619,\n",
       " 'asleep': 620,\n",
       " 'travelling': 621,\n",
       " 'weigh': 622,\n",
       " '100': 623,\n",
       " 'gain': 624,\n",
       " '...': 625,\n",
       " 'wisp': 626,\n",
       " 'saddest': 627,\n",
       " 'copper': 628,\n",
       " 'jest': 629,\n",
       " 'bones': 630,\n",
       " 'purpose': 631,\n",
       " 'built': 632,\n",
       " 'mysteries': 633,\n",
       " 'disposed': 634,\n",
       " 'overweight': 635,\n",
       " 'sorts': 636,\n",
       " 'checking': 637,\n",
       " 'plot': 638,\n",
       " 'plant': 639,\n",
       " 'addict': 640,\n",
       " 'there': 641,\n",
       " 'sample': 642,\n",
       " 'lecture': 643,\n",
       " 'problem': 644,\n",
       " 'did': 645,\n",
       " 'lily': 646,\n",
       " 'alienated': 647,\n",
       " 'hardworking': 648,\n",
       " 'yumi': 649,\n",
       " 'enormous': 650,\n",
       " 'fasten': 651,\n",
       " 'mike': 652,\n",
       " 'sullen': 653,\n",
       " 'sounds': 654,\n",
       " 'adjust': 655,\n",
       " 'stone': 656,\n",
       " 'spicy': 657,\n",
       " 'ticket': 658,\n",
       " 'washes': 659,\n",
       " 'bargaining': 660,\n",
       " 'absorbed': 661,\n",
       " 'immense': 662,\n",
       " 'replied': 663,\n",
       " 'discourage': 664,\n",
       " 'inform': 665,\n",
       " 'blank': 666,\n",
       " 'experience': 667,\n",
       " 'execute': 668,\n",
       " 'spread': 669,\n",
       " 'symptom': 670,\n",
       " 'cleverest': 671,\n",
       " 'allow': 672,\n",
       " 'shrinks': 673,\n",
       " 'beck': 674,\n",
       " 'hold': 675,\n",
       " 'nod': 676,\n",
       " 'granted': 677,\n",
       " 'approval': 678,\n",
       " 'exercising': 679,\n",
       " 'menu': 680,\n",
       " 'smiths': 681,\n",
       " 'sank': 682,\n",
       " 'prophecy': 683,\n",
       " 'reconciled': 684,\n",
       " 'gritty': 685,\n",
       " 'bound': 686,\n",
       " 'soothe': 687,\n",
       " 'kidney': 688,\n",
       " 'taking': 689,\n",
       " 'short-handed': 690,\n",
       " 'salary': 691,\n",
       " 'hum': 692,\n",
       " 'approach': 693,\n",
       " 'understood': 694,\n",
       " 'stiff': 695,\n",
       " 'hanged': 696,\n",
       " 'ntt': 697,\n",
       " 'cranky': 698,\n",
       " 'shows': 699,\n",
       " 'headache': 700,\n",
       " 'economy': 701,\n",
       " 'issue': 702,\n",
       " 'terrace': 703,\n",
       " 'claim': 704,\n",
       " 'burnt': 705,\n",
       " 'speed': 706,\n",
       " 'translate': 707,\n",
       " 'decay': 708,\n",
       " 'thrown': 709,\n",
       " 'athlete': 710,\n",
       " 'instructive': 711,\n",
       " 'bell': 712,\n",
       " 'school': 713,\n",
       " 'rakugoka': 714,\n",
       " 'refund': 715,\n",
       " 'mustn': 716,\n",
       " 'huh': 717,\n",
       " 'safely': 718,\n",
       " 'coals': 719,\n",
       " 'seeks': 720,\n",
       " 'spoken': 721,\n",
       " 'chauffeur': 722,\n",
       " 'bliss': 723,\n",
       " 'disco': 724,\n",
       " 'troubled': 725,\n",
       " 'deceived': 726,\n",
       " 'shivering': 727,\n",
       " 'hospital': 728,\n",
       " 'bedtime': 729,\n",
       " 'faultless': 730,\n",
       " 'blessed': 731,\n",
       " 'beer': 732,\n",
       " 'penniless': 733,\n",
       " 'else': 734,\n",
       " 'hospitality': 735,\n",
       " 'ocean': 736,\n",
       " 'expensive': 737,\n",
       " 'handwriting': 738,\n",
       " 'newspaper': 739,\n",
       " 'keen': 740,\n",
       " 'soft-hearted': 741,\n",
       " 'demanded': 742,\n",
       " 'rule': 743,\n",
       " 'kleenex': 744,\n",
       " 'seventy': 745,\n",
       " 'workshop': 746,\n",
       " 'chopped': 747,\n",
       " 'wished': 748,\n",
       " 'tani': 749,\n",
       " 'nikon': 750,\n",
       " 'barber': 751,\n",
       " 'fetched': 752,\n",
       " 'standstill': 753,\n",
       " 'change': 754,\n",
       " 'considered': 755,\n",
       " 'pure': 756,\n",
       " 'prevailed': 757,\n",
       " 'misbehave': 758,\n",
       " 'bargained': 759,\n",
       " 'browns': 760,\n",
       " 'prejudiced': 761,\n",
       " 'ushered': 762,\n",
       " 'generally': 763,\n",
       " 'maybe': 764,\n",
       " 'relying': 765,\n",
       " 'help': 766,\n",
       " 'breath': 767,\n",
       " 'fluent': 768,\n",
       " 'autumn': 769,\n",
       " 'soaked': 770,\n",
       " 'appropriate': 771,\n",
       " 'pals': 772,\n",
       " 'aboard': 773,\n",
       " 'unfriendly': 774,\n",
       " 'lacked': 775,\n",
       " 'pendant': 776,\n",
       " 'expresses': 777,\n",
       " 'wonders': 778,\n",
       " 'strangely': 779,\n",
       " 'beethoven': 780,\n",
       " 'politician': 781,\n",
       " 'agreeing': 782,\n",
       " 'honor': 783,\n",
       " 'shorten': 784,\n",
       " 'cathy': 785,\n",
       " 'repainted': 786,\n",
       " 'nowhere': 787,\n",
       " 'fork': 788,\n",
       " 'disappointed': 789,\n",
       " 'far': 790,\n",
       " 'tune-up': 791,\n",
       " 'killing': 792,\n",
       " 'unhappy': 793,\n",
       " 'conquest': 794,\n",
       " 'like': 795,\n",
       " 'wind': 796,\n",
       " 'notice': 797,\n",
       " 'mostly': 798,\n",
       " 'helen': 799,\n",
       " 'committed': 800,\n",
       " 'shhh': 801,\n",
       " 'cockroaches': 802,\n",
       " 'closer': 803,\n",
       " 'comic': 804,\n",
       " 'lengths': 805,\n",
       " 'angrily': 806,\n",
       " 'add': 807,\n",
       " 'passing': 808,\n",
       " 'to': 809,\n",
       " 'couldn': 810,\n",
       " 'forth': 811,\n",
       " 'intention': 812,\n",
       " 'offence': 813,\n",
       " 'curtis': 814,\n",
       " 'tony': 815,\n",
       " 'concert': 816,\n",
       " 'oral': 817,\n",
       " 'freeze': 818,\n",
       " 'properly': 819,\n",
       " 'likely': 820,\n",
       " 'port': 821,\n",
       " 'feed': 822,\n",
       " 'adore': 823,\n",
       " 'overthrown': 824,\n",
       " 'practicing': 825,\n",
       " 'puzzled': 826,\n",
       " 'pride': 827,\n",
       " 'patricia': 828,\n",
       " 'child': 829,\n",
       " 'text': 830,\n",
       " 'hair': 831,\n",
       " 'computers': 832,\n",
       " 'post-office': 833,\n",
       " 'designer': 834,\n",
       " 'pestering': 835,\n",
       " 'washing': 836,\n",
       " 'enamored': 837,\n",
       " 'mend': 838,\n",
       " 'concentrated': 839,\n",
       " 'rate': 840,\n",
       " 'cake': 841,\n",
       " 'workman': 842,\n",
       " 'sashimi': 843,\n",
       " 'soon': 844,\n",
       " 'attached': 845,\n",
       " 'computer': 846,\n",
       " 'fidget': 847,\n",
       " 'fatigue': 848,\n",
       " 'much': 849,\n",
       " 'teaching': 850,\n",
       " 'expectation': 851,\n",
       " 'railroad': 852,\n",
       " 'borne': 853,\n",
       " 'lamp': 854,\n",
       " 'annoys': 855,\n",
       " 'miho': 856,\n",
       " 'starry': 857,\n",
       " 'unburdened': 858,\n",
       " 'imp': 859,\n",
       " 'pleasures': 860,\n",
       " 'reaction': 861,\n",
       " 'hills': 862,\n",
       " 'nightmares': 863,\n",
       " 'disappoint': 864,\n",
       " 'intervals': 865,\n",
       " 'intently': 866,\n",
       " 'done': 867,\n",
       " 'vogue': 868,\n",
       " 'lying': 869,\n",
       " 'albums': 870,\n",
       " 'heaven': 871,\n",
       " 'examples': 872,\n",
       " 'veins': 873,\n",
       " 'forward': 874,\n",
       " 'inaugurated': 875,\n",
       " 'aggressive': 876,\n",
       " 'childhood': 877,\n",
       " 'overflow': 878,\n",
       " 'cars': 879,\n",
       " 'emergency': 880,\n",
       " 'italian': 881,\n",
       " 'hailing': 882,\n",
       " 'until': 883,\n",
       " 'apt': 884,\n",
       " 'sewed': 885,\n",
       " 'priced': 886,\n",
       " 'minister': 887,\n",
       " 'simon': 888,\n",
       " 'faces': 889,\n",
       " 'cocktail': 890,\n",
       " 'grades': 891,\n",
       " 'occupied': 892,\n",
       " 'greek': 893,\n",
       " 'practising': 894,\n",
       " 'presents': 895,\n",
       " 'monkeys': 896,\n",
       " 'humiliated': 897,\n",
       " 'mom': 898,\n",
       " 'meant': 899,\n",
       " 'abundant': 900,\n",
       " 'acquaintances': 901,\n",
       " 'convinced': 902,\n",
       " 'roll': 903,\n",
       " 'diligent': 904,\n",
       " 'if': 905,\n",
       " 'scholar': 906,\n",
       " 'ax': 907,\n",
       " 'errand': 908,\n",
       " 'glare': 909,\n",
       " 'ming': 910,\n",
       " 'blonde': 911,\n",
       " 'partner': 912,\n",
       " '1943': 913,\n",
       " 'sweep': 914,\n",
       " 'address': 915,\n",
       " 'rescue': 916,\n",
       " 'commanded': 917,\n",
       " 'indispensable': 918,\n",
       " 'people': 919,\n",
       " 'clogged': 920,\n",
       " 'shinjuku': 921,\n",
       " 'overeat': 922,\n",
       " 'hour': 923,\n",
       " 'am': 924,\n",
       " 'indulged': 925,\n",
       " 'stride': 926,\n",
       " 'election': 927,\n",
       " 'play': 928,\n",
       " 'apprehensions': 929,\n",
       " 'deepest': 930,\n",
       " 'congratulate': 931,\n",
       " 'bicycle': 932,\n",
       " 'announcers': 933,\n",
       " 'vital': 934,\n",
       " 'severe': 935,\n",
       " 'persist': 936,\n",
       " 'busy': 937,\n",
       " 'term': 938,\n",
       " 'clothes': 939,\n",
       " 'compromised': 940,\n",
       " 'countenance': 941,\n",
       " 'cancer': 942,\n",
       " 'behooves': 943,\n",
       " 'title': 944,\n",
       " 'blows': 945,\n",
       " 'tip': 946,\n",
       " 'nicer': 947,\n",
       " 'muggy': 948,\n",
       " 'downstairs': 949,\n",
       " 'error': 950,\n",
       " 'learned': 951,\n",
       " 'flames': 952,\n",
       " 'interrupted': 953,\n",
       " 'graduation': 954,\n",
       " 'bag': 955,\n",
       " 'initiative': 956,\n",
       " 'alternative': 957,\n",
       " 'study': 958,\n",
       " 'excellent': 959,\n",
       " 'laundry': 960,\n",
       " 'removed': 961,\n",
       " 'essays': 962,\n",
       " 'nora': 963,\n",
       " 'wrist': 964,\n",
       " 'stumped': 965,\n",
       " 'hated': 966,\n",
       " 'harshly': 967,\n",
       " 'alone': 968,\n",
       " 'resist': 969,\n",
       " 'duties': 970,\n",
       " 'kanji': 971,\n",
       " 'weighs': 972,\n",
       " 'inclined': 973,\n",
       " 'written': 974,\n",
       " 'haughty': 975,\n",
       " 'guitar': 976,\n",
       " 'oppressed': 977,\n",
       " 'applied': 978,\n",
       " 'seize': 979,\n",
       " 'contemplated': 980,\n",
       " 'bitten': 981,\n",
       " 'kicks': 982,\n",
       " 'engagement': 983,\n",
       " 'following': 984,\n",
       " 'safety': 985,\n",
       " 'snow': 986,\n",
       " 'putting': 987,\n",
       " 'carrying': 988,\n",
       " 'land': 989,\n",
       " 'sixty': 990,\n",
       " 'purely': 991,\n",
       " 'immigrants': 992,\n",
       " 'campus': 993,\n",
       " 'fitted': 994,\n",
       " 'car': 995,\n",
       " 'walked': 996,\n",
       " 'value': 997,\n",
       " 'roads': 998,\n",
       " 'becomes': 999,\n",
       " 'shinano': 1000,\n",
       " 'chemistry': 1001,\n",
       " 'lines': 1002,\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_vocab_size = len(e_w2i)\n",
    "j_vocab_size = len(j_w2i)\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "m = tf.cast(tf.not_equal(x, 0), tf.float32)   #padding部分にマスク\n",
    "d = tf.placeholder(tf.int32, [None, None], name='d')\n",
    "d_in = d[:, :-1]\n",
    "\n",
    "d_out = d[:, 1:]\n",
    "d_out_one_hot = tf.one_hot(d_out, depth=j_vocab_size, dtype=tf.float32)\n",
    "\n",
    "def f_props(layers, x):\n",
    "    for layer in layers:\n",
    "        x = layer.f_prop(x)\n",
    "    return x\n",
    "\n",
    "encoder = [\n",
    "    Embedding(e_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, hid_dim, m)\n",
    "]\n",
    "\n",
    "h_enc, c_enc = f_props(encoder, x)\n",
    "\n",
    "decoder_pre = [\n",
    "    Embedding(j_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, hid_dim, tf.ones_like(d_in, dtype='float32'), h_0=h_enc[:, -1, :], c_0=c_enc[:, -1, :]),\n",
    "]\n",
    "\n",
    "decoder_post = [\n",
    "    Dense(hid_dim, j_vocab_size, tf.nn.softmax)\n",
    "]\n",
    "\n",
    "h_dec, c_dec = f_props(decoder_pre, d_in)\n",
    "y = f_props(decoder_post, h_dec)\n",
    "\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(d_out_one_hot * tf.log(tf.clip_by_value(y, 1e-10, 1.0)), axis=[1, 2]))\n",
    "\n",
    "train = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_lens = [len(com) for com in train_X]\n",
    "sorted_train_indexes = sorted(range(len(train_X_lens)), key=lambda x: -train_X_lens[x])\n",
    "\n",
    "train_X = [train_X[ind] for ind in sorted_train_indexes]\n",
    "train_y = [train_y[ind] for ind in sorted_train_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d9d8d82c7c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_y_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_X_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_y_mb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#n_epochs = 10\n",
    "n_epochs = 1\n",
    "batch_size = 128\n",
    "n_batches = len(train_X) // batch_size #186\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    train_costs = []\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        train_X_mb = np.array(pad_sequences(train_X[start:end], padding='post', value=0))\n",
    "        train_y_mb = np.array(pad_sequences(train_y[start:end], padding='post', value=0))\n",
    "\n",
    "        _, train_cost = sess.run([train, cost], feed_dict={x: train_X_mb, d: train_y_mb})\n",
    "        train_costs.append(train_cost)\n",
    "\n",
    "    # valid\n",
    "    valid_X_mb = np.array(pad_sequences(valid_X, padding='post', value=0))\n",
    "    valid_y_mb = np.array(pad_sequences(valid_y, padding='post', value=0))\n",
    "\n",
    "    valid_cost = sess.run(cost, feed_dict={x: valid_X_mb, d: valid_y_mb})\n",
    "    print('EPOCH: %i, Training cost: %.3f, Validation cost: %.3f' % (epoch+1, np.mean(train_cost), valid_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "翻訳文の生成には`while`ループを使うので, まず`tf`におけるwhileループの実装である`tf.while_loop`について説明します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5.1. `tf.while_loop`関数  \\[[link\\]](https://www.tensorflow.org/api_docs/python/tf/while_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主な引数は以下のとおりです.\n",
    "\n",
    "- 第1引数 `cond`: `True` or `False` を返す関数 (正確にはcallable)\n",
    "- 第2引数 `body`: 各iterationで実行する関数 (正確にはcallable)\n",
    "- 第3引数 `loop_vars`: `cond`及び`body`に最初に渡される変数\n",
    "\n",
    "`cond`で指定された関数の戻り値が`True`である限り`body`で指定された関数を実行し続けます. そして, `loop_vars`で指定された全ての変数に対して, 最後のiteration後の値を返します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば, 入力が5未満であるかぎり1ずつ足す処理を実行したい場合, コードは次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "def cond(z):\n",
    "    return z < 5\n",
    "\n",
    "def body(z):\n",
    "    return z + 1\n",
    "\n",
    "with g0.as_default():\n",
    "    z = tf.constant(0)\n",
    "\n",
    "    res = tf.while_loop(cond, body, [z])\n",
    "\n",
    "with tf.Session(graph=g0) as sess_g0:\n",
    "    print(sess_g0.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また, `tf.while_loop`の各iteration後の変数の`shape`はデフォルトでは同じであることが指定されています. なので, 各iteration後の`shape`が変化する場合は, この条件を緩和する必要があります.\n",
    "\n",
    "たとえば, 上の1ずつ足すプログラムですべてのiteration後の値を保持して返したい場合, 各iterationの戻り値は\n",
    "\n",
    "```\n",
    "[1], [1, 2], [1, 2, 3], [1, 2, 3, 4], ...\n",
    "```\n",
    "となっていくので, それぞれの`shape`は,\n",
    "```\n",
    "(1,), (2,), (3,), (4,), ...\n",
    "```\n",
    "と変化していきます. つまり, この例ではベクトルの次元数が変化していくので, `shape_invariants`で`shape`を`[None]` (実際は`tf.TensorShape([None])`と指定します.\n",
    "\n",
    "具体的なコードは次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "def cond(z):\n",
    "    return z[-1] < 5\n",
    "\n",
    "def body(z):\n",
    "    return tf.concat([z, z[-1:]+1], axis=0)\n",
    "\n",
    "with g1.as_default():\n",
    "    z = tf.zeros(1)\n",
    "\n",
    "    res = tf.while_loop(\n",
    "        cond,\n",
    "        body,\n",
    "        [z],\n",
    "        shape_invariants=[tf.TensorShape([None])]\n",
    "    )\n",
    "\n",
    "with tf.Session(graph=g1) as sess_g1:\n",
    "    print(sess_g1.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2x1行列に対して同じような操作をしたい場合は次のようになります. この場合各iteration後の行列の`shape`は\n",
    "```\n",
    "(2, 1), (2, 2), (2, 3), (2, 4), ...\n",
    "```\n",
    "と列数のみ変化していくので, `shape_invariants`には`tf.TensorShape([2, None])`と指定します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "\n",
    "def cond(z):\n",
    "    return tf.reduce_sum(z[:, -1]) < 5*2\n",
    "\n",
    "def body(z):\n",
    "    return tf.concat([z, z[:, -1:]+1], axis=1)\n",
    "\n",
    "with g2.as_default():\n",
    "    z = tf.zeros([2, 1])\n",
    "\n",
    "    res = tf.while_loop(\n",
    "        cond,\n",
    "        body,\n",
    "        [z],\n",
    "        shape_invariants=[tf.TensorShape([2, None])]\n",
    "    )\n",
    "\n",
    "with tf.Session(graph=g2) as sess_g2:\n",
    "    print(sess_g2.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "詳細は公式のドキュメントを参照してください.\n",
    "\n",
    "- tf.while_loop: https://www.tensorflow.org/api_docs/python/tf/while_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. グラフの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未知のデータに対してEncoder-Decoderモデルを適用するとき, 正解ラベル$d$はわからないので, 代わりに前のステップで予測した単語を各ステップでの入力とします. そして, 系列の終わりを表す単語 (`</s>`) が出力されるまで繰り返します.\n",
    "\n",
    "具体的には, $h_{t-1}, c_{t-1}, y_{t-1}$を入力として$y_t$を受け取る操作を, バッチ内の全てのサンプルにおける$y_t$が`</s>`となるまで続けます.\n",
    "\n",
    "毎iterationで1つのステップについてのみ順伝播を計算すれば良いので, ここで各クラスの`f_prop_test`関数を使用します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_0 = tf.constant(0)\n",
    "y_0 = tf.placeholder(tf.int32, [None, None], name='y_0')\n",
    "h_0 = tf.placeholder(tf.float32, [None, None], name='h_0')\n",
    "c_0 = tf.placeholder(tf.float32, [None, None], name='c_0')\n",
    "f_0 = tf.cast(tf.zeros_like(y_0[:, 0]), dtype=tf.bool) # バッチ内の各サンプルに対して</s>が出たかどうかのflag\n",
    "f_0_size = tf.reduce_sum(tf.ones_like(f_0, dtype=tf.int32))\n",
    "max_len = tf.placeholder(tf.int32, name='max_len') # iterationの繰り返し回数の限度\n",
    "\n",
    "def f_props_test(layers, x_t):\n",
    "    for layer in layers:\n",
    "        x_t = layer.f_prop_test(x_t)\n",
    "    return x_t\n",
    "\n",
    "def cond(t, h_t, c_t, y_t, f_t):\n",
    "    num_true = tf.reduce_sum(tf.cast(f_t, tf.int32)) # Trueの数\n",
    "    unfinished = tf.not_equal(num_true, f_0_size)\n",
    "    return tf.logical_and(t+1 < max_len, unfinished)\n",
    "\n",
    "def body(t, h_tm1, c_tm1, y, f_tm1):\n",
    "    y_tm1 = y[:, -1]\n",
    "\n",
    "    decoder_pre[1].h_0 = h_tm1\n",
    "    decoder_pre[1].c_0 = c_tm1\n",
    "    h_t, c_t = f_props_test(decoder_pre, y_tm1)\n",
    "    y_t = tf.cast(tf.argmax(f_props_test(decoder_post, h_t), axis=1), tf.int32)\n",
    "\n",
    "    y = tf.concat([y, y_t[:, np.newaxis]], axis=1)\n",
    "\n",
    "    f_t = tf.logical_or(f_tm1, tf.equal(y_t, 1)) # flagの更新\n",
    "\n",
    "    return [t+1, h_t, c_t, y, f_t]\n",
    "\n",
    "res = tf.while_loop(\n",
    "    cond,\n",
    "    body,\n",
    "    loop_vars=[t_0, h_0, c_0, y_0, f_0],\n",
    "    shape_invariants=[\n",
    "        t_0.get_shape(),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. 初期値$h_0, c_0, y_0$の獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_X_mb = pad_sequences(valid_X, padding='post', value=0)\n",
    "_y_0 = np.zeros_like(valid_X, dtype='int32')[:, np.newaxis]\n",
    "_h_enc, _c_enc = sess.run([h_enc, c_enc], feed_dict={x: valid_X_mb})\n",
    "_h_0 = _h_enc[:, -1, :]\n",
    "_c_0 = _c_enc[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, pred_y, _ = sess.run(res, feed_dict={\n",
    "    y_0: _y_0,\n",
    "    h_0: _h_0,\n",
    "    c_0: _c_0,\n",
    "    max_len: 100\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. 生成例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の文: i got up at five that morning .\n",
      "生成文された文: は 彼 は 何 を し て い る 。\n",
      "正解文: 私 は その 朝 ５ 時 に 起き た 。\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "\n",
    "origy = valid_X[num][1:-1]\n",
    "predy = list(pred_y[num])\n",
    "truey = valid_y[num][1:-1]\n",
    "\n",
    "print('元の文:', ' '.join([e_i2w[com] for com in origy]))\n",
    "print('生成文された文:', ' '.join([j_i2w[com] for com in predy[1:predy.index(2)]]))\n",
    "print('正解文:', ' '.join([j_i2w[com] for com in truey]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(valid_X, dtype='int32')[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
