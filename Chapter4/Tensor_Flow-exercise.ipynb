{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flowを用いたロジスティック回帰，MLP実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.  Tensorflowとは？\n",
    "\n",
    "Googleが提供する機械学習用のフレームワーク．\n",
    "機械学習用のフレームワークは他にもたくさん存在するが，Tensorflowは現在世界で最も使用されているフレームワークであると言われている．\n",
    "\n",
    "pythonによって書くが、内部はC++やcudaによって書かれている．\n",
    "\n",
    "### 'define and run'という形式をとり、まず計算グラフを定義し、それに対してデータを流すという使い方となっている．\n",
    "\n",
    "※tensor flowインストールの際は，conda内でpipを使うとcondaのデータ破損原因になるので，$ conda install tensorflow　または，anaconda-navigator->Environmentsより，uninstall項目からtensorflowをinstallするのがよいと思われます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.　計算グラフの構築と実行\n",
    "\n",
    "計算グラフを構築するためにTensorflow側が用意している型を用いる必要がある.<br>\n",
    "\n",
    "Tensorflowが用意している種類と使い方は以下のとおり．\n",
    "1. tf.constant ... ハイパーパラメータなど，実行前から形(shape)の決まった定数に用いる．\n",
    "2. tf.placeholder ... データの入力など，実行するまでデータのshapeはわからないが変わらないデータを入れるときに用いる(初期化不要)．例えば，データセットの大きさは実行するまでわからない．\n",
    "3. tf.Variable ... ネットワークの重みなど，学習中に値が変わる最適化対象を入れる(初期化必要)． "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0　計算グラフの実行方法\n",
    "計算グラフを構築するだけでは，実際に計算は行われない．<br>\n",
    "計算を実行して値を評価するためには， TensorflowのSessionを作成する必要がある．<br>\n",
    "例えば，$x$という値の出力が欲しい時は，その値をSessionのrunメソッドに渡してあげる．<br>\n",
    "具体的には以下のように書けば良い．\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1　まずはtf.constant(定数)を用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1)\n",
    "y = tf.constant(2)\n",
    "\n",
    "add_op = tf.add(x, y)\n",
    "print(x,y)\n",
    "print(add_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで表示された結果は定義された計算グラフについての情報で、実際に計算は行われていないことに注意．\n",
    "- 以下のように計算グラフを実行させて値を確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(add_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x_, y_, add_op_ = sess.run([x, y, add_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('x is ',x_)\n",
    "print('y is ', y_)\n",
    "print('x + y = ',add_op_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "足し算掛け算は以下のようにも書ける．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1)\n",
    "y = tf.constant(2)\n",
    "\n",
    "## 足し算掛け算は+,*で書いて良い\n",
    "add_op = x+y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x_, y_, add_op_ = sess.run([x, y, add_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('x is ',x_)\n",
    "print('y is ', y_)\n",
    "print('x + y = ',add_op_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb.show_graph(tf.get_default_graph().as_graph_def()) # 前のが残っている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2　tf.placeholderを用いる(データを流す用)\n",
    "\n",
    "placeholderは初期化不要の変数だが、intかfloatか指定する必要がある．\n",
    "- tf.float32\n",
    "- tf.int32\n",
    "\n",
    "評価対象の変数の計算のために必要なデータの入力はsess.run内のfeed_dict引数内で行うことができる．<br>\n",
    "feed_dictで渡す変数は一つとは限らないので，辞書型で渡す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.placeholder(tf.int32)\n",
    "x = tf.constant(5)\n",
    "op = data*x\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result1 = sess.run(op, feed_dict={data: 5})\n",
    "    result2 = sess.run(op, feed_dict={data: 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('5*5=',result1)\n",
    "print('5*10=',result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3　tf.Variableを用いる(変数用)\n",
    "\n",
    "- 実行前に全てのVariableは初期化する必要がある．\n",
    "    - sess.run(tf.global_variables_initializer())で一度に初期化できる\n",
    "- Variableへの代入はtf.assignを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var1 = tf.Variable(0)\n",
    "const1 = tf.constant(2)\n",
    "\n",
    "add_op = var1+const1\n",
    "# Variableへの代入はassignを用いる\n",
    "var1 = tf.assign(var1, add_op)\n",
    "print (var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())  #絶対必要！\n",
    "    # var1が毎回更新されている\n",
    "    print(sess.run([var1]))\n",
    "    print(sess.run([var1]))\n",
    "    print(sess.run([var1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2，MNIST分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今回はKNNで扱った手書き文字分類を実装する．前回ロジスティック回帰で扱った問題は二値分類であったが，今回は多クラス分類問題となる．出力層の出力はsoftmax関数にし、誤差関数は交差エントロピー誤差を用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データのロード（比較的時間がかかる）\n",
    "mnist = fetch_mldata('MNIST original', data_home='./data/')\n",
    "\n",
    "# data : 画像データ， target : 正解ラベル\n",
    "X, T = mnist.data, mnist.target\n",
    "\n",
    "# 画像データは0~255の数値となっているので，0~1の値に変換\n",
    "X = X / 255.\n",
    "\n",
    "#　訓練データとテストデータに分ける\n",
    "X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.2)\n",
    "\n",
    "# データのサイズ\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "# ラベルデータをint型に統一し，学習に使いやすいようにone-hot-vectorに変換\n",
    "#T_train = np.eye(10)[T_train.astype(\"int\")]\n",
    "#T_test = np.eye(10)[T_test.astype(\"int\")]\n",
    "T_train = to_categorical(T_train)\n",
    "T_test = to_categorical(T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('訓練データのサイズは', N_train)\n",
    "print ('テストデータのサイズは', N_test)\n",
    "print ('画像データのshapeは', X_train.shape)\n",
    "print ('ラベルデータのshapeは', T_train.shape)\n",
    "print ('ラベルデータの数値の例：')\n",
    "print (T_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot-vectorとは？\n",
    "たとえば$a$が，0~4の整数のみを含むベクトルだとわかっている時に，各行の数字に該当する列の要素のみを1にし，その他を0にする．\n",
    "$$\n",
    "\\begin{equation*}\n",
    "a=\n",
    "\\begin{pmatrix}\n",
    "3\\\\\n",
    "1\\\\\n",
    "4\\\\\n",
    "2\\\\\n",
    "0\n",
    "\\end{pmatrix}\\to\n",
    "a\\_onehot = \n",
    "\\begin{pmatrix}\n",
    "0, 0, 0, 1, 0\\\\\n",
    "0, 1, 0, 0, 0\\\\\n",
    "0, 0, 0, 0, 1\\\\\n",
    "0, 0, 1, 0, 0\\\\\n",
    "1, 0, 0, 0, 0\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "学習における正解ラベルデータは，one-hot-vectorで表されることが多い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # n_in : 入力次元数\n",
    "        # n_out : 出力次元数\n",
    "        self.W = tf.Variable(tf.zeros([$$$, $$$])) # 重み\n",
    "        self.b = tf.Variable(tf.zeros($$$)) # バイアス\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ### WRITE ME ###\n",
    "        y = $$$ # Forward Propagation\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#グラフの初期化\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#必要なパラメータの定義\n",
    "# Learning rate (学習率)\n",
    "lr = 0.7\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 25\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x = tf.placeholder(tf.float32, [None, $$$]) # 28*28次元 \n",
    "t = tf.placeholder(tf.float32, [None, $$$]) # 10クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "# 入力次元数：784，　出力次元数：10\n",
    "model =MLP($$$, $$$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y : predictionの結果\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(t*tf.log(tf.clip_by_value(y, 1e-10, 1.0)), axis=1 )) # tf.log(0)によるnanを防ぐ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 精度評価\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += np.mean(loss) * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.3f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バッチ学習について\n",
    "バッチ学習とは，大量のデータを扱う際に，データをいくつかの小さな集合にわけ，その集合ごとに勾配を計算し，パラメータを更新する手法である．今までは全てのデータを参照してから，一度重みの更新を行っていたため，大量の計算を行っても重み更新が少しづつしか行えない問題があった．バッチ学習はこの問題を解決する手法である．今回の問題では，56000個の訓練データを100個ずつのバッチに分けて，そのバッチごとに勾配計算する操作を一回の操作とし，その操作をepoch回数分(25回)分計算を回して重み更新を行う．つまり，$560×25=14000回$の重み更新を行っている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPクラスについて\n",
    "以下のような要件のネットワークを構築する．\n",
    "```\n",
    "    入力 : x　\n",
    "-> Fully connected layer 1 (input : x, outputの次元数 : 256, 活性化関数 : relu関数)\n",
    "-> Fully connected layer 2 (input : layer1の出力， outputの次元数 : 256, 活性化関数 : relu関数)\n",
    "-> Fully connected layer 3 (input : layer2の出力， outputの次元数 : 10)\n",
    "-> 出力 : out\n",
    "```\n",
    "\n",
    "<details>\n",
    "    <summary>ヒント</summary>\n",
    "    <div><br>\n",
    "    - TensorflowでFully connected layerはtf.layers.dense (inputs, units, activation=None)で呼ぶことができる．\n",
    "    <br>\n",
    "    - inputs : 入力データ\n",
    "    <br>\n",
    "    - units :  outputの次元数\n",
    "    <br>\n",
    "    - activation : 活性化関数の種類（デフォルトでは無し）\n",
    "    <br>\n",
    "    - relu関数はTensorflowでtf.nn.reluと表される．\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(x):\n",
    "    ### TODO\n",
    "    layer_1 =  tf.layers.dense(x, 256, activation=tf.nn.relu)\n",
    "    layer_2 =  tf.layers.dense(layer_1, 256, activation=tf.nn.relu)\n",
    "    out = tf.layers.dense(layer_2, 10, activation=tf.nn.softmax)\n",
    "    ### TODO\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# パラメータ\n",
    "# Learning rate (学習率)\n",
    "lr = 0.1\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 25\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 100\n",
    "\n",
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28*28次元 \n",
    "t = tf.placeholder(tf.float32, [None, 10]) # 10クラス\n",
    "\n",
    "# MLPクラスのモデルを用いてpredictionを行う\n",
    "y = MLP(x)\n",
    "\n",
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(t*tf.log(tf.clip_by_value(y, 1e-10, 1.0)),axis=1)) # tf.log(0)によるnanを防ぐ\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# test用\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "            \n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += np.mean(loss) * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.5f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Test Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# < Tensorflow 応用編 >\n",
    "\n",
    "Tensorflowには学習コードをさらに抽象化する<strong>Estimator</strong>というものがある．<br>\n",
    "Estimatorを用いるとEstimator.train()を呼ぶだけでepoch数だけfor文を書くなどの手間が省けるため，コードの可読性が上がり，<br>\n",
    "実用でも広く使われている．\n",
    "\n",
    "## Estimatorの使い方\n",
    "Estimatorの使用方法について大まかなコードの流れの例を説明したものが以下のサンプルコードである．\n",
    "```python\n",
    "estimator = tf.estimator.Estimator(model_fn)\n",
    "estimator.train(train_input_fn)\n",
    "result = estimator.evaluate(test_input_fn)\n",
    "```\n",
    "model_fnにモデルの内容（ネットワーク構造，ロス関数，optimizerの設定）を書き，Estimatorに渡す．<br>\n",
    "Estimatorは訓練用のデータの情報が入っているtrain_input_fnを引数としてtrainメソッドを呼ぶことで訓練を行う．<br>\n",
    "さらに，テスト用のデータの情報が入っているtest_input_fnを引数としてevaluateメソッドを呼ぶことでテストを行い，学習データになかったデータに対する汎用性を評価する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimatorでの実装時に用いられる関数はone-hot-vectorではなくクラスラベルをそのまま入力することが多い\n",
    "t_train = np.argmax(T_train, axis=1)\n",
    "t_test = np.argmax(T_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意！\n",
    "loss関数の名前シリーズ\n",
    "\n",
    "\n",
    "- tf.nn.tf.nn.softmax_cross_entropy\n",
    "- tf.nn.tf.nn.softmax_cross_entropy_with_logits\n",
    "- tf.nn.sparse_softmax_cross_entropy\n",
    "- tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "\n",
    "どれつかったらいいの...\n",
    "\n",
    "### 重要\n",
    "\n",
    "with_logitsがついているものはこの関数は内部でソフトマックスも計算するから、ニューラルネットワークの出力をそのまま入力してね、という意味です。\n",
    "\n",
    "sparseが付いていると、自動でinputをone-hotにしてくれるそうです。はい、便利！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    # モデルを定義する\n",
    "    net = features['x']\n",
    "    for units in params['hidden_units']:\n",
    "        # 入力がnetで出力がunits個，活性化関数がrelu関数である全結合層\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # netを入力としてクラス分のunit数に出力することでlogitsを計算する\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "    \n",
    "    # ロスの定義\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # 推定結果を計算する\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    \n",
    "    # 正解率の計算方法を指定する\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_classes)\n",
    "    metrics = {'accuracy': accuracy}\n",
    "  \n",
    "    # evaluationモードのとき\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # lossを最小化するオプティマイザを定義する\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "     \n",
    "    # trainモードのとき\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 100\n",
    "\n",
    "    # 訓練データ用\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_train},\n",
    "        y=t_train,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=5,\n",
    "        shuffle=True)\n",
    "    \n",
    "    # テストデータ用\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_test},\n",
    "        y=t_test,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    # model functionを用いてestimatorを定義する\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        params={\n",
    "            'hidden_units': [256, 256], #  256ノード持つ2つの隠れ層\n",
    "            'n_classes': 10, # モデルは結果を3つのクラスから選ぶ\n",
    "        })\n",
    "     \n",
    "    # モデルを訓練\n",
    "    estimator.train(input_fn=train_input_fn)\n",
    "    \n",
    "    # モデルの評価\n",
    "    eval_result = estimator.evaluate(input_fn=eval_input_fn)\n",
    "    \n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flowを用いた回帰，二値分類の問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前回MLP問題として扱った回帰，二値分類の問題をTensor Flowを用いて実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $f(x)=x+0.3\\sin(2\\pi x)$ に沿って生成したノイズを含むデータ点から，近似曲線を推定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なライブラリのインポート "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_toy_dataset(func, n=100):\n",
    "    #サイズ(n,1)の配列に0~1の乱数を格納\n",
    "    x = np.random.uniform(size=(n, 1))\n",
    "    #関数の返り値にノイズを発生\n",
    "    t = func(x) + np.random.uniform(-0.1, 0.1, size=(n, 1))\n",
    "    return x, t\n",
    "\n",
    "#f(x)=x+0.3sin(2πx)を定義\n",
    "def func(x):\n",
    "        return x + 0.3 * np.sin(2 * np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#学習データ\n",
    "x, t = create_toy_dataset(func)\n",
    "x_size=x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#観測データと正解の関数\n",
    "\n",
    "plt.scatter(x, t, alpha=0.5, label=\"observation\")\n",
    "x_test = np.linspace(0, 1, 1000)[:, np.newaxis]\n",
    "plt.plot(x_test, func(x_test), color=\"blue\", label=\"$x+0.3\\sin(2\\pi x)$\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(x):\n",
    "    ### TODO\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# パラメータ\n",
    "# Learning rate (学習率)\n",
    "lr = 0.1\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 5000\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 10\n",
    "\n",
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x_ = tf.placeholder(tf.float32, [None, 1]) # 1次元 \n",
    "t_ = tf.placeholder(tf.float32, [None, 1]) # 1次元\n",
    "\n",
    "# MLPクラスのモデルを用いてpredictionを行う\n",
    "y_ = MLP(x_)\n",
    "\n",
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "cost = tf.reduce_mean(tf.square(y_ - t_))\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# test用\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(t_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(x_size)\n",
    "    \n",
    "        for i in range(0, x_size, batchsize):\n",
    "            \n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = x[perm[i:i+batchsize]]\n",
    "            t_batch = t[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x_:X_batch, t_:t_batch})\n",
    "            sum_loss += np.mean(loss) * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss /x_size\n",
    "        print('Train loss %.5f  ' %(loss))\n",
    "        \n",
    "        # Test model\n",
    "        #print (\"Test Accuracy: %.3f\"%(sess.run(accuracy,feed_dict={x_: x, t_: t})))\n",
    "    pred,x_data=sess.run([y_,x_], feed_dict={x_:x, t_:t})\n",
    "    plt.scatter(x, t, alpha=0.5, label=\"observation\")    \n",
    "    x_test = np.linspace(0, 1, 1000)[:, np.newaxis]\n",
    "    plt.plot(x_test, func(x_test), color=\"blue\", label=\"$x+0.3\\sin(2\\pi x)$\")\n",
    "    plt.scatter(x_data, pred, color=\"red\", label=\"regression\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 二値分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_toy_dataset2():\n",
    "    x = np.random.uniform(-1., 1., size=(1000, 2))\n",
    "    labels = (np.prod(x, axis=1) > 0).astype(np.float)\n",
    "    return x, labels.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#データ生成\n",
    "x, labels = create_toy_dataset2()\n",
    "print(x.shape)\n",
    "x_size=x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#可視化\n",
    "print(x.shape)\n",
    "print(labels.shape)\n",
    "colors = [\"blue\", \"red\"]\n",
    "plt.scatter(x[:, 0], x[:, 1], c=[colors[int(label)] for label in labels])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(x):\n",
    "    ### TODO\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# パラメータ\n",
    "# Learning rate (学習率)\n",
    "lr = 0.001\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 1000\n",
    "\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 1000\n",
    "\n",
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x_ = tf.placeholder(tf.float32, [None, 2]) # 1次元 \n",
    "t_ = tf.placeholder(tf.float32, [None, 1]) # 1次元\n",
    "\n",
    "# MLPクラスのモデルを用いてpredictionを行う\n",
    "y_ = MLP(x_)\n",
    "\n",
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "cost = tf.reduce_sum(-t_ * tf.log(y_) - (1 - t_) * tf.log(1 - y_))\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# test用\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(t_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(x_size)\n",
    "    \n",
    "        for i in range(0, x_size, batchsize):\n",
    "            \n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = x[perm[i:i+batchsize]]\n",
    "            t_batch = labels[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x_:X_batch, t_:t_batch})\n",
    "            \n",
    "            sum_loss += np.mean(loss) * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss /x_size\n",
    "        print('Train loss %.5f  ' %(loss))\n",
    "\n",
    "        # Test model\n",
    "        #print (\"Test Accuracy: %.3f\"%(accuracy.eval(feed_dict={x_: x, t_: labels})))\n",
    "    \n",
    "    X_test, Y_test = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n",
    "    x_test = np.array([X_test, Y_test]).transpose(1, 2, 0).reshape(-1, 2)    \n",
    "    probs = sess.run(y_, feed_dict={x_:x_test, t_:labels})\n",
    "    Probs = probs.reshape(100, 100)\n",
    "    levels = np.linspace(0, 1, 11)\n",
    "    plt.contourf(X_test, Y_test, Probs, levels, alpha=0.5)\n",
    "    plt.colorbar()\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
