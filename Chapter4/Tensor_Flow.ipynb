{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flowを用いたロジスティック回帰，MLP実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.  Tensorflowとは？\n",
    "\n",
    "Googleが提供する機械学習用のフレームワーク．\n",
    "機械学習用のフレームワークは他にもたくさん存在するが，Tensorflowは現在世界で最も使用されているフレームワークであると言われている．\n",
    "\n",
    "pythonによって書くが、内部はC++やcudaによって書かれている．\n",
    "\n",
    "### 'define and run'という形式をとり、まず計算グラフを定義し、それに対してデータを流すという使い方となっている．\n",
    "\n",
    "※tensor flowインストールの際は，conda内でpipを使うとcondaのデータ破損原因になるので，$ conda install tensorflow　または，anaconda-navigator->Environmentsより，uninstall項目からtensorflowをinstallするのがよいと思われます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.　計算グラフの構築と実行\n",
    "\n",
    "計算グラフを構築するためにTensorflow側が用意している型を用いる必要がある.<br>\n",
    "\n",
    "Tensorflowが用意している種類と使い方は以下のとおり．\n",
    "1. tf.constant ... ハイパーパラメータなど，実行前から形(shape)の決まった定数に用いる．\n",
    "2. tf.placeholder ... データの入力など，実行するまでデータのshapeはわからないが変わらないデータを入れるときに用いる(初期化不要)．例えば，データセットの大きさは実行するまでわからない．\n",
    "3. tf.Variable ... ネットワークの重みなど，学習中に値が変わる最適化対象を入れる(初期化必要)． "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0　計算グラフの実行方法\n",
    "計算グラフを構築するだけでは，実際に計算は行われない．<br>\n",
    "計算を実行して値を評価するためには， TensorflowのSessionを作成する必要がある．<br>\n",
    "例えば，$x$という値の出力が欲しい時は，その値をSessionのrunメソッドに渡してあげる．<br>\n",
    "具体的には以下のように書けば良い．\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1　まずはtf.constant(定数)を用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(), dtype=int32) Tensor(\"Const_3:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1)\n",
    "y = tf.constant(2)\n",
    "\n",
    "add_op = tf.add(x, y)\n",
    "print(x,y)\n",
    "print(add_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで表示された結果は定義された計算グラフについての情報で、実際に計算は行われていないことに注意．\n",
    "- 以下のように計算グラフを実行させて値を確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(add_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x_, y_, add_op_ = sess.run([x, y, add_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is  1\n",
      "y is  2\n",
      "x + y =  3\n"
     ]
    }
   ],
   "source": [
    "print('x is ',x_)\n",
    "print('y is ', y_)\n",
    "print('x + y = ',add_op_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "足し算掛け算は以下のようにも書ける．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1)\n",
    "y = tf.constant(2)\n",
    "\n",
    "## 足し算掛け算は+,*で書いて良い\n",
    "add_op = x+y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x_, y_, add_op_ = sess.run([x, y, add_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is  1\n",
      "y is  2\n",
      "x + y =  3\n"
     ]
    }
   ],
   "source": [
    "print('x is ',x_)\n",
    "print('y is ', y_)\n",
    "print('x + y = ',add_op_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2　tf.placeholderを用いる(データを流す用)\n",
    "\n",
    "placeholderは初期化不要の変数だが、intかfloatか指定する必要がある．\n",
    "- tf.float32\n",
    "- tf.int32\n",
    "\n",
    "評価対象の変数の計算のために必要なデータの入力はsess.run内のfeed_dict引数内で行うことができる．<br>\n",
    "feed_dictで渡す変数は一つとは限らないので，辞書型で渡す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.placeholder(tf.int32)\n",
    "x = tf.constant(5)\n",
    "op = data*x\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result1 = sess.run(op, feed_dict={data: 5})\n",
    "    result2 = sess.run(op, feed_dict={data: 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5*5= 25\n",
      "5*10= 50\n"
     ]
    }
   ],
   "source": [
    "print('5*5=',result1)\n",
    "print('5*10=',result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3　tf.Variableを用いる(変数用)\n",
    "\n",
    "- 実行前に全てのVariableは初期化する必要がある．\n",
    "    - sess.run(tf.global_variables_initializer())で一度に初期化できる\n",
    "- Variableへの代入はtf.assignを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Assign:0\", shape=(), dtype=int32_ref)\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable(0)\n",
    "const1 = tf.constant(2)\n",
    "\n",
    "add_op = var1+const1\n",
    "# Variableへの代入はassignを用いる\n",
    "var1 = tf.assign(var1, add_op)\n",
    "print (var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[4]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "# var1が毎回更新されている\n",
    "    print(sess.run([var1]))\n",
    "    print(sess.run([var1]))\n",
    "    print(sess.run([var1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2，ロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今回はKNNで扱った手書き文字分類を実装する．前回ロジスティック回帰で扱った問題は二値分類であったが，今回は多クラス分類問題となる．その際，活性化関数および誤差関数が変更される点に注意．\n",
    "### 二値分類→活性化関数：シグモイド関数，目的関数：エントロピー誤差<br>多クラス分類問題→活性化関数：ソフトマックス関数，目的関数：交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データのロード（比較的時間がかかる）\n",
    "mnist = fetch_mldata('MNIST original', data_home='./data/')\n",
    "\n",
    "# data : 画像データ， target : 正解ラベル\n",
    "X, T = mnist.data, mnist.target\n",
    "\n",
    "# 画像データは0~255の数値となっているので，0~1の値に変換\n",
    "X = X / 255.\n",
    "\n",
    "#　訓練データとテストデータに分ける\n",
    "X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.2)\n",
    "\n",
    "# データのサイズ\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "# ラベルデータをint型に統一し，学習に使いやすいようにone-hot-vectorに変換\n",
    "T_train = np.eye(10)[T_train.astype(\"int\")]\n",
    "T_test = np.eye(10)[T_test.astype(\"int\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データのサイズは 56000\n",
      "テストデータのサイズは 14000\n",
      "画像データのshapeは (56000, 784)\n",
      "ラベルデータのshapeは (56000, 10)\n",
      "ラベルデータの数値の例：\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print ('訓練データのサイズは', N_train)\n",
    "print ('テストデータのサイズは', N_test)\n",
    "print ('画像データのshapeは', X_train.shape)\n",
    "print ('ラベルデータのshapeは', T_train.shape)\n",
    "print ('ラベルデータの数値の例：')\n",
    "print (T_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot-vectorとは？\n",
    "たとえば$a$が，0~4の整数のみを含むベクトルだとわかっている時に，各行の数字に該当する列の要素のみを1にし，その他を0にする．\n",
    "$$\n",
    "\\begin{equation*}\n",
    "a=\n",
    "\\begin{pmatrix}\n",
    "3\\\\\n",
    "1\\\\\n",
    "4\\\\\n",
    "2\\\\\n",
    "0\n",
    "\\end{pmatrix}\\to\n",
    "a\\_onehot = \n",
    "\\begin{pmatrix}\n",
    "0, 0, 0, 1, 0\\\\\n",
    "0, 1, 0, 0, 0\\\\\n",
    "0, 0, 0, 0, 1\\\\\n",
    "0, 0, 1, 0, 0\\\\\n",
    "1, 0, 0, 0, 0\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "学習における正解ラベルデータは，one-hot-vectorで表されることが多い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰クラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # n_in : 入力次元数\n",
    "        # n_out : 出力次元数\n",
    "        self.W = tf.Variable(tf.zeros([n_in, n_out])) # 重み\n",
    "        self.b = tf.Variable(tf.zeros(n_out)) # バイアス\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ### TODO ###\n",
    "        y = tf.sigmoid(tf.matmul(x,self.W)+self.b) # Forward Propagation\n",
    "        ### TODO ###\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#グラフの初期化\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#必要なパラメータの定義\n",
    "# Learning rate (学習率)\n",
    "lr = 0.7\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 25\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28*28次元 \n",
    "t = tf.placeholder(tf.float32, [None, 10]) # 10クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "# 入力次元数：784，　出力次元数：10\n",
    "model = LogisticRegression(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y : predictionの結果\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y)\n",
    "cost = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度評価\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | Train loss 1.685 | Accuracy: 0.876\n",
      "epoch 1 | Train loss 1.612 | Accuracy: 0.886\n",
      "epoch 2 | Train loss 1.598 | Accuracy: 0.890\n",
      "epoch 3 | Train loss 1.590 | Accuracy: 0.893\n",
      "epoch 4 | Train loss 1.585 | Accuracy: 0.893\n",
      "epoch 5 | Train loss 1.581 | Accuracy: 0.897\n",
      "epoch 6 | Train loss 1.578 | Accuracy: 0.897\n",
      "epoch 7 | Train loss 1.576 | Accuracy: 0.898\n",
      "epoch 8 | Train loss 1.574 | Accuracy: 0.899\n",
      "epoch 9 | Train loss 1.572 | Accuracy: 0.901\n",
      "epoch 10 | Train loss 1.571 | Accuracy: 0.900\n",
      "epoch 11 | Train loss 1.569 | Accuracy: 0.903\n",
      "epoch 12 | Train loss 1.568 | Accuracy: 0.903\n",
      "epoch 13 | Train loss 1.567 | Accuracy: 0.904\n",
      "epoch 14 | Train loss 1.566 | Accuracy: 0.903\n",
      "epoch 15 | Train loss 1.565 | Accuracy: 0.903\n",
      "epoch 16 | Train loss 1.564 | Accuracy: 0.904\n",
      "epoch 17 | Train loss 1.564 | Accuracy: 0.905\n",
      "epoch 18 | Train loss 1.563 | Accuracy: 0.905\n",
      "epoch 19 | Train loss 1.562 | Accuracy: 0.906\n",
      "epoch 20 | Train loss 1.562 | Accuracy: 0.906\n",
      "epoch 21 | Train loss 1.561 | Accuracy: 0.906\n",
      "epoch 22 | Train loss 1.560 | Accuracy: 0.906\n",
      "epoch 23 | Train loss 1.560 | Accuracy: 0.906\n",
      "epoch 24 | Train loss 1.559 | Accuracy: 0.907\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += np.mean(loss) * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.3f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPクラスについて\n",
    "以下のような要件のネットワークを構築する．\n",
    "```\n",
    "    入力 : x　\n",
    "-> Fully connected layer 1 (input : x, outputの次元数 : 256, 活性化関数 : relu関数)\n",
    "-> Fully connected layer 2 (input : layer1の出力， outputの次元数 : 256, 活性化関数 : relu関数)\n",
    "-> Fully connected layer 3 (input : layer2の出力， outputの次元数 : 10)\n",
    "-> 出力 : out\n",
    "```\n",
    "\n",
    "<details>\n",
    "    <summary>ヒント</summary>\n",
    "    <div><br>\n",
    "    - TensorflowでFully connected layerはtf.layers.dense (inputs, units, activation=None)で呼ぶことができる．\n",
    "    <br>\n",
    "    - inputs : 入力データ\n",
    "    <br>\n",
    "    - units :  outputの次元数\n",
    "    <br>\n",
    "    - activation : 活性化関数の種類（デフォルトでは無し）\n",
    "    <br>\n",
    "    - relu関数はTensorflowでtf.nn.reluと表される．\n",
    "    </div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(x):\n",
    "    ### TODO\n",
    "    layer_1 =  tf.layers.dense(x, 256, activation=tf.nn.relu)\n",
    "    layer_2 =  tf.layers.dense(layer_1, 256, activation=tf.nn.relu)\n",
    "    out = tf.layers.dense(layer_2, 10, activation=tf.nn.softmax)\n",
    "    ### TODO\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# パラメータ\n",
    "# Learning rate (学習率)\n",
    "lr = 0.1\n",
    "# epoch数 （学習回数）\n",
    "n_epoch = 25\n",
    "# ミニバッチ学習における1バッチのデータ数\n",
    "batchsize = 100\n",
    "\n",
    "# 入力\n",
    "# placeholderを用いると，データのサイズがわからないときにとりあえずNoneとおくことができる．\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28*28次元 \n",
    "t = tf.placeholder(tf.float32, [None, 10]) # 10クラス\n",
    "\n",
    "# MLPクラスのモデルを用いてpredictionを行う\n",
    "y = MLP(x)\n",
    "\n",
    "# 目的関数:softmax cross entropy\n",
    "# 入力：labels->正解ラベル， logits：predictionの結果\n",
    "# 出力：softmax cross entropyで計算された誤差\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y)\n",
    "cost = tf.reduce_mean(xentropy)\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# test用\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | Train loss 1.82402 | Test Accuracy: 0.823\n",
      "epoch 1 | Train loss 1.64486 | Test Accuracy: 0.837\n",
      "epoch 2 | Train loss 1.62949 | Test Accuracy: 0.845\n",
      "epoch 3 | Train loss 1.62146 | Test Accuracy: 0.852\n",
      "epoch 4 | Train loss 1.61572 | Test Accuracy: 0.854\n",
      "epoch 5 | Train loss 1.61116 | Test Accuracy: 0.857\n",
      "epoch 6 | Train loss 1.60720 | Test Accuracy: 0.859\n",
      "epoch 7 | Train loss 1.60408 | Test Accuracy: 0.862\n",
      "epoch 8 | Train loss 1.60120 | Test Accuracy: 0.863\n",
      "epoch 9 | Train loss 1.59873 | Test Accuracy: 0.867\n",
      "epoch 10 | Train loss 1.59642 | Test Accuracy: 0.867\n",
      "epoch 11 | Train loss 1.59420 | Test Accuracy: 0.869\n",
      "epoch 12 | Train loss 1.59228 | Test Accuracy: 0.871\n",
      "epoch 13 | Train loss 1.59046 | Test Accuracy: 0.873\n",
      "epoch 14 | Train loss 1.58883 | Test Accuracy: 0.873\n",
      "epoch 15 | Train loss 1.58723 | Test Accuracy: 0.874\n",
      "epoch 16 | Train loss 1.58572 | Test Accuracy: 0.876\n",
      "epoch 17 | Train loss 1.57573 | Test Accuracy: 0.950\n",
      "epoch 18 | Train loss 1.50881 | Test Accuracy: 0.955\n",
      "epoch 19 | Train loss 1.50181 | Test Accuracy: 0.960\n",
      "epoch 20 | Train loss 1.49848 | Test Accuracy: 0.962\n",
      "epoch 21 | Train loss 1.49567 | Test Accuracy: 0.964\n",
      "epoch 22 | Train loss 1.49352 | Test Accuracy: 0.963\n",
      "epoch 23 | Train loss 1.49174 | Test Accuracy: 0.965\n",
      "epoch 24 | Train loss 1.49016 | Test Accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        # Training\n",
    "        sum_loss = 0\n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "\n",
    "        for i in range(0, N_train, batchsize):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batchsize]]\n",
    "            t_batch = T_train[perm[i:i+batchsize]]\n",
    "\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x:X_batch, t:t_batch})\n",
    "            sum_loss += np.mean(loss) * X_batch.shape[0]\n",
    "\n",
    "        loss = sum_loss / N_train\n",
    "        print('Train loss %.5f | ' %(loss), end=\"\")\n",
    "\n",
    "        # Test model\n",
    "        print (\"Test Accuracy: %.3f\"%(accuracy.eval(feed_dict={x: X_test, t: T_test})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
