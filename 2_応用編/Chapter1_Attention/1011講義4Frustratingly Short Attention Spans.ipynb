{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frustratingly Short Attention Spans\n",
    "\n",
    "ICLR2017\n",
    "\n",
    "https://arxiv.org/pdf/1702.04521.pdf\n",
    "\n",
    "Michał Daniluk, Tim Rocktaschel, Johannes Welbl & Sebastian Riedel<br>\n",
    "Department of Computer Science<br>\n",
    "University College London<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手法の紹介は３つ\n",
    "\n",
    "基本的には**次の単語を予測するタスク**における提案\n",
    "\n",
    "前の隠れ層にAttentionを加えて隠れ層の出力の精度向上を図る\n",
    "\n",
    "LSTMの次のtimestepの出力の作り方\n",
    "\n",
    "Attentionの精度向上手法の提案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1. ATTENTION FOR NEURAL LANGUAGE MODELING\n",
    "\n",
    "従来のAttentionの用い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/model1.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "$$M_t = tanh(W^Y[h_{t-L}...h_{t-1}]+W^h[h_t...h_T])$$\n",
    "$$\\alpha_t=softmax(w^TM_t)$$\n",
    "$$r_t=[h_{t-L}...h_{t-1}]\\alpha^T$$\n",
    "$$h^*_t=tanh(W^rr_t+W^xh_t)$$\n",
    "-->\n",
    "\n",
    "<img src ='img/eq0.png' width=400>\n",
    "そして最後に\n",
    "### $$y_t=softmax(W^*h_t^*+b)$$\n",
    "として次の単語を出力する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つまり$h_i$というのは以下の３つの役割を担っている\n",
    "\n",
    "\n",
    "\n",
    "- **Attentionの計算** 　　数式１行目\n",
    "- Attentionに応じて足し合わされる**特徴量**(context vector)　　　数式3行目\n",
    "- **次のtokenの推定**　　　数式4行目\n",
    "\n",
    "\n",
    "## Main Idea\n",
    "それはさすがにoverloadなタスクだ<br>\n",
    "役割を別々に分けてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2.KEY-VALUE SEPARATION\n",
    "Attentionの重みを計算する用の隠れ層と、合算する隠れ層は分ける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='img/model2.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/eq1_1.png' width=100>\n",
    "<img src ='img/eq1.png' width=400>\n",
    "そして最後に\n",
    "$$y_t=softmax(W^*h_t^*+b)$$\n",
    "として次の単語を出力する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3.KEY-VALUE-PREDICT SEPARATION\n",
    "\n",
    "隠れ層を３つのpartに分けた。\n",
    "- Attention weightの計算\n",
    "- context vectorの計算\n",
    "- 次の単語の予測用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='img/model3.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/eq2_1.png' width=100>\n",
    "<img src ='img/eq2.png' width=400>\n",
    "そして最後に\n",
    "$$y_t=softmax(W^*h_t^*+b)$$\n",
    "として次の単語を出力する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipediaのコーパスにおいて、次の単語を予測するというタスク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Wightの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia記事からランダムサンプリングした、Key-Value-PredictモデルにおけるAttention weightの可視化\n",
    "<img src='img/posia.png' witdth=300>\n",
    "\n",
    "各手法における重みの分布の平均\n",
    "<img src='img/posib.png' witdth=500>\n",
    "注)[RM(+tM-g)](https://arxiv.org/abs/1601.01272)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionを可視化してみると、**ほとんど後ろの方（2~5step）しか見ていない**ことがわかった\n",
    "\n",
    "\n",
    "そこで以下のidea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-4.N-Gram Recurrent Neural Networks\n",
    "\n",
    "直近だけでAttentionを計算しようというアイディア\n",
    "\n",
    "後ろN-1個を見るとすると、N-1個に隠れ層を分割する\n",
    "\n",
    "実験結果では後ろ5個を見るのが最適だったとのこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/model4.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/eq3.png' width=500>\n",
    "そして最後に\n",
    "$$y_t=softmax(W^*h_t^*+b)$$\n",
    "として次の単語を出力する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='img/result.png' width=600>\n",
    "値はperplexityで、低いほどよい<br>\n",
    "凝ったAttentionモデルとそんなに変わらないという結果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 議論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次の単語を予測するようなタスクで、複雑なAttentionは本当にいる？<br>\n",
    "というより長期間を扱えている？\n",
    "\n",
    "長期間の依存関係を見るためにはもっと研究が必要\n",
    "\n",
    "---\n",
    "\n",
    "実験に用いたデータがそもそも長期間の依存関係を見なくてもいい問題だった？\n",
    "\n",
    "という意見も"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
