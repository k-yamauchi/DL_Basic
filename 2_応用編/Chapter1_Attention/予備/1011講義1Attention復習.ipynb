{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention復習\n",
    "\n",
    "参考)http://deeplearning.hatenablog.com/entry/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Global Attention(Seq2Seqより)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luong et al., 2015のGlobal attentionモデル\n",
    "\n",
    "- \"Effective Approaches to Attention-based Neural Machine Translation\", Minh-Thang Luong et al., EMNLP 2015 https://arxiv.org/abs/1508.04025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機構\n",
    "<img src=\"img/attention-1.png\" width=\"1000mm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoderの各ステップにおける計算の手順\n",
    "\n",
    "Encoderの各ステップの隠れ層を\n",
    "\n",
    "$$\n",
    "    \\bar{h} = \\{\\bar{h}_1, \\bar{h}_2, \\ldots, \\bar{h}_s, \\ldots, \\bar{h}_S\\}\n",
    "$$\n",
    "\n",
    "Decoderの各ステップの隠れ層を\n",
    "\n",
    "$$\n",
    "    h = \\{h_1, h_2, \\ldots, h_t, \\ldots, h_T\\}\n",
    "$$\n",
    "\n",
    "とします."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Layerの計算手順は以下のようになります.\n",
    "\n",
    "1. まずRNN (or LSTM, GRU, etc.) により, 隠れ層ベクトルを計算します.\n",
    "$$\n",
    "    h_t = \\mathrm{RNN}(h_{t-1}, x_t)\n",
    "$$\n",
    "2. 次に, 入力系列のどのステップに注目するのかの**重み**$a_t(s)$を, score関数 (後述) により計算します.\n",
    "$$\n",
    "    a_t(s) = \\frac{\\exp(\\mathrm{score}(\\bar{h}_s, h_t))}{\\sum^S_{s'=1}\\exp(\\mathrm{score}(\\bar{h}_s, h_t))}\n",
    "$$\n",
    "3. 2.で計算した重みをもとに, Encoderの各ステップの隠れ層に対する重み付き平均ベクトル (**文脈ベクトル**) $c_t$ を計算します.\n",
    "$$\n",
    "    c_t = \\sum^S_{s=1} a_t(s) \\bar{h}_s\n",
    "$$\n",
    "4. 3.で計算した文脈ベクトルと1.で計算した隠れ層ベクトルから, 新しい出力ベクトルを計算します.\n",
    "$$\n",
    "    \\tilde{h}_t = \\tanh(W_h h_t + W_c c_t + b)\n",
    "$$\n",
    "5. 新しい出力ベクトル$\\tilde{h}_t$をもとに各単語の出力確率を計算します.\n",
    "$$\n",
    "    y_t = \\mathrm{softmax}(W_{out}\\tilde{h}_t + b_{out})\n",
    "$$\n",
    "\n",
    "論文では$\\tilde{h}_t$が次のタイムステップのLSTMにfeedされる方法も示されています. 他の論文でもそのようにしているものも多いです. その場合, Attention層とLSTM層は分けずに, 同じクラスで書くことになります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### score関数について\n",
    "\n",
    "Encoderのどこに注目するかを決める関数です. score関数には以下の様なものが提案されています.\n",
    "\n",
    "主に３種類\n",
    "- Dot Product\n",
    "$$attention\\_score(h_j^{(src)},h_t^{(dest)})=h_j^{(src)\\mathrm{T}}h_t^{(dest)}$$\n",
    "- Bilinear function\n",
    "$$attention\\_score(h_j^{(src)},h_t^{(dest)})=h_j^{(src)\\mathrm{T}}W_ah_t^{(dest)}$$\n",
    "- Multi Layer Perceptron\n",
    "$$attention\\_score(h_j^{(src)},h_t^{(dest)})=w_{a2}^{\\mathrm{T}}tanh(W_{a1} [h_j^{(src)};h_t^{(dest)}]$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "他にもたくさんあるらしい..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Captioing with Attention\n",
    "https://arxiv.org/pdf/1502.03044.pdf\n",
    "<img src=\"img/attention-2.png\" size=\"1000mm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず224x224x3の画像をEncoder (CNN) で特徴量マップ$u$に落とし込み, それを元にDecoder (LSTM + Attention) でキャプションを生成していきます.\n",
    "\n",
    "特徴mapは平滑化して用いる(そのためscoreの計算には$h_t^{\\mathrm{T}} W_a \\bar{h}_s$などを用いる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self, e_hid_dim, d_hid_dim, out_dim, h_enc, m_h_enc):\n",
    "        self.W_cc = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=[e_hid_dim, out_dim]).astype('float32'), name='W_cc')\n",
    "        self.W_ch = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=[d_hid_dim, out_dim]).astype('float32'), name='W_ch')\n",
    "        self.W_a  = tf.Variable(rng.uniform(low=-0.08, high=0.08, size=[e_hid_dim, d_hid_dim]).astype('float32'), name='W_a')\n",
    "        self.b    = tf.Variable(np.zeros([out_dim]).astype('float32'), name='b')\n",
    "        self.h_enc = h_enc\n",
    "        self.m_h_enc = m_h_enc\n",
    "\n",
    "    def f_prop(self, h_dec):\n",
    "        # self.h_enc: [batch_size(i), enc_length(j), e_hid_dim(k)]\n",
    "        # self.W_a  : [e_hid_dim(k), d_hid_dim(l)]\n",
    "        # -> h_enc: [batch_size(i), enc_length(j), d_hid_dim(l)]\n",
    "        h_enc = tf.einsum('ijk,kl->ijl', self.h_enc, self.W_a)\n",
    "        \n",
    "        # h_dec: [batch_size(i), dec_length(j), d_hid_dim(k)]\n",
    "        # h_enc: [batch_size(i), enc_length(l), d_hid_dim(k)]\n",
    "        # -> score: [batch_size(i), dec_length(j), enc_length(l)]\n",
    "        score = tf.einsum('ijk,ilk->ijl', h_dec, h_enc) # Attention score\n",
    "        \n",
    "        # score  : [batch_size, dec_length, enc_length]\n",
    "        # m_h_enc: [batch_size, enc_length] -> [batch_size, np.newaxis, enc_length]\n",
    "        score = score * self.m_h_enc[:, np.newaxis, :] # Mask\n",
    "        \n",
    "        # encoderのステップにそって正規化する\n",
    "        self.a = tf.nn.softmax(score) # Attention weight\n",
    "        \n",
    "        # self.a  : [batch_size(i), dec_length(j), enc_length(k)]\n",
    "        # self.enc: [batch_size(i), enc_length(k), e_hid_dim(l)]\n",
    "        # -> c: [batch_size(i), dec_length(j), e_hid_dim(l)]\n",
    "        c = tf.einsum('ijk,ikl->ijl', self.a, self.h_enc) # Context vector\n",
    "        \n",
    "        return tf.nn.tanh(tf.einsum('ijk,kl->ijl', c, self.W_cc) + tf.einsum('ijk,kl->ijl', h_dec, self.W_ch) + self.b)\n",
    "    \n",
    "    def f_prop_test(self, h_dec_t):\n",
    "        # self.h_enc: [batch_size(i), enc_length(j), e_hid_dim(k)]\n",
    "        # self.W_a  : [e_hid_dim(k), d_hid_dim(l)]\n",
    "        # -> h_enc: [batch_size(i), enc_length(j), d_hid_dim(l)]\n",
    "        h_enc = tf.einsum('ijk,kl->ijl', self.h_enc, self.W_a)\n",
    "        \n",
    "        # h_dec_t: [batch_size(i), d_hid_dim(j)]\n",
    "        # h_enc  : [batch_size(i), enc_length(k), d_hid_dim(j)]\n",
    "        # -> score: [batch_size(i), enc_length(k)]\n",
    "        score = tf.einsum('ij,ikj->ik', h_dec_t, h_enc) # Attention score\n",
    "        \n",
    "        # score       : [batch_size(i), enc_length(k)]\n",
    "        # self.m_h_enc: [batch_size(i), enc_length(k)]\n",
    "        score = score * self.m_h_enc # Mask\n",
    "        \n",
    "        self.a = tf.nn.softmax(score) # Attention weight\n",
    "        \n",
    "        # self.a    : [batch_size(i), enc_length(j)]\n",
    "        # self.h_enc: [batch_size(i), enc_length(j), e_hid_dim(k)]\n",
    "        # -> c: [batch_size(i), e_hid_dim(k)]\n",
    "        c = tf.einsum('ij,ijk->ik', self.a, self.h_enc) # Context vector\n",
    "\n",
    "        return tf.nn.tanh(tf.matmul(c, self.W_cc) + tf.matmul(h_dec_t, self.W_ch) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/showattendtell.jpg' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word level or Sentence Level Attention for Documetn Classification\n",
    "\n",
    "文書分類にAttentionを用いたHierarchical Attention Network(HAN)\n",
    "\n",
    "- 単語レベルでのAttention(どの単語について着目するか)\n",
    "- 文章レベルでのAttention(どの文章について着目するか)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/han-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Word Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional(重要)なLSTMもしくはGRUで一文ごとにEncode\n",
    "<img src='img/haneq1.png' width=200>\n",
    "<img src='img/haneq2.png' width=150>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Word Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/haneq3.png' width=200>\n",
    "最初に$h_{it}$について1層のMLPに入力し、$u_{it}$(word-level context vectorを得る)\n",
    "\n",
    "Attention Weightの計算には他の隠れ層のword-level context vectorと1層のMLPによって計算される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Sentence Encoder\n",
    "文章レベルについても同様\n",
    "<img src='img/haneq4.png' width=200>\n",
    "<img src='img/haneq5.png' width=150>\n",
    "\n",
    "### 4. Sentence Attention\n",
    "文章レベルについても同様\n",
    "<img src='img/haneq6.png' width=200>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attentionの可視化\n",
    "<img src ='img/hanvis.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Key Value Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的なAttention（内積）を用いた場合\n",
    "${Attention(Target, \\, Source) = Softmax(Target \\cdot Source^T) \\cdot Source}$\n",
    "\n",
    "\n",
    "これのうち$Target$を$query$  (検索クエリ) と見做し，$Source$ を $Key$ と $Value$ に分離する．\n",
    "\n",
    "<img src ='img/KV.png'>\n",
    "この時 Key と Value は各 key と各 value が一対一対応する key-value ペアの配列，つまり辞書オブジェクトとして機能する．\n",
    "\n",
    "query と Key の内積は query と各 key の類似度を測り，softmax で正規化した注意の重み (Attention Weight) は query に一致した key の位置を表現する．注意の重みと Value の内積は key の位置に対応する value を加重和として取り出す操作である．\n",
    "\n",
    "つまりAttentionとは query (検索クエリ) に一致する key を索引し，対応する value を取り出す操作であり，これは辞書オブジェクトの機能と同じである．例えば一般的な Encoder-Decoder の注意は，エンコーダのすべての隠れ層 (情報源) Value から query に関連する隠れ層 (情報) value を注意の重みの加重和として取り出すことである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関連論文\n",
    "\n",
    "Frustratingly Short Attention Spans in Neural Language Modeling [Michał Daniluk, arXiv, ICLR, 2017/02]\n",
    "\n",
    "<img src ='img/Frustrating.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AttentionのQ,K,Vがどこから（Source? Target? Self?）くるのかによって**Source-Target Attention**と**Self- Attention**\n",
    "<img src ='img/attentioncomparison.png' width=600>\n",
    "\n",
    "**Source-Target-Attention** では Key と Value はエンコーダの隠れ層 (Source) から来て，Query はデコーダの隠れ層 (Target) から来る．一般的な Encoder-Decoder の注意はこちらである．前述のとおり Source は Memory とも呼ばれ，Key と Value は Memory を 2 つに分離したものと解釈できる．\n",
    "\n",
    "**Self-Attention** では Query,Key,Value は全て同じ場所 (Self) から来る．例えばエンコーダの Query,Key,Value はすべて下の隠れ層から来る．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関連論文\n",
    "[Attention is All You Need](https://arxiv.org/abs/1706.03762)<br>\n",
    "[Łukasz Kaiser et al., arXiv, 2017/06]\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "Transformer では内積注意を縮小付き内積注意 (Scaled Dot-Product Attention) と呼称する．通常の内積注意と同じく query をもとに key-value ペアの配列から加重和として value を取り出す操作であるが，Q と K の内積をスケーリング因子 ${\\sqrt{d_{k}}}$ で除算する．\n",
    "\n",
    "また，query の配列は 1 つの行列 Q にまとめて同時に内積注意を計算する (従来通り key と value の配列も K,V にまとめる)．\n",
    "\n",
    "縮小付き内積注意は次式によって表される．\n",
    "\n",
    "<img src ='img/shikiattention.png' width=300>\n",
    "\n",
    "dk が小さい場合，スケーリング因子がなくても内積注意は加法注意と同様に機能する．しかし dk が大きい場合，スケーリング因子がないと加法注意の方がうまく機能する．内積注意は内積が大きくなりすぎて，逆伝播の softmax の勾配が極端に小さくなることが原因である．\n",
    "<img src ='img/singlehead.png' width=200>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,Dot,Concatenate\n",
    "from keras.activations import softmax\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SingleHeadSelfAttentionMudule():\n",
    "\n",
    "    x= Input(shape=(20,512))\n",
    "    \n",
    "    #QKV\n",
    "    Q = Dense(64,name='Q')(x)\n",
    "    K = Dense(64,name='K')(x)\n",
    "    V = Dense(64,name='V')(x)\n",
    "    \n",
    "    score = Dot(-1,name='score')([Q,K])\n",
    "    attention_weights=Lambda(lambda x: softmax(x/8,axis=2), name='attention_weights')(score)\n",
    "        \n",
    "    weighted_V = Dot(1,name='weighted_V')([V,attention_weights])\n",
    "\n",
    "    model = Model(inputs=x, outputs=weighted_V)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 20, 512)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Q (Dense)                        (None, 20, 64)        32832       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "K (Dense)                        (None, 20, 64)        32832       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "score (Dot)                      (None, 20, 20)        0           Q[0][0]                          \n",
      "                                                                   K[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "V (Dense)                        (None, 20, 64)        32832       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Lambda)       (None, 20, 20)        0           score[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "weighted_V (Dot)                 (None, 64, 20)        0           V[0][0]                          \n",
      "                                                                   attention_weights[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 98,496\n",
      "Trainable params: 98,496\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SingleHeadSelfAttentionMudule()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
