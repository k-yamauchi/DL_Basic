{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentionとは\n",
    "連続したデータを扱う際に過去の重要なポイントに着目する(=Attention)ための手法\n",
    "\n",
    "最終時刻の結果しか用いないより全部使った方がいい\n",
    "\n",
    "実際LSTMも最終時刻だけより、全時刻の平均を用いる方が少し良いらしい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentionを行う流れ\n",
    "\n",
    "- これまでの隠れ層に対し、何らかの形でスコアリングする\n",
    "- スコアをsoftmax関数にかけて正規化する\n",
    "- 得られた重みについてそれぞれの隠れ層を加重平均する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Machine Translation + Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Attention Model\n",
    "<img src='img/encdecattention.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ a_t = \\frac{score(h_t,\\bar{h_s})}{\\sum_{s'}{score(h_t,\\bar{h_s'}})}$$\n",
    "\n",
    "<img src ='img/score.png' width=300>\n",
    "\n",
    "### $$ c_t = \\sum_s \\alpha_t(s)\\bar{h_s} $$\n",
    "\n",
    "### $$ \\tilde{h_t} = tanh{W_c[c_t;h_t]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Attention\n",
    "論文：[Effective Approaches to Attention-based Neural Machine Translation](http://aclweb.org/anthology/D15-1166)\n",
    "<img src ='img/localattention.png' width =600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それぞれの単語についてscoreを計算しなくてもよくない？\n",
    "\n",
    "Globalは長い文になると計算コスト増える\n",
    "\n",
    "Local Attentionは更に見る場所を限定する  [$p_t-D,p_t+D$]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. $p_t$の計算 \n",
    "<img src ='img/p_t.png' width =300>\n",
    "$p_t$は[0,S]の値を取り、どこの単語を中心に見たらいいかを判断する\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.scoreの計算\n",
    "\n",
    "Global Attentionの時と同様だが、$p_t$から前後D個のみ計算\n",
    "\n",
    "つまり$2D+1$個のAttentionを計算\n",
    "\n",
    "<img src ='img/localattention3.png' width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.　正規分布を仮定\n",
    "<img src ='img/seikibunpu2.png' width =400>\n",
    "\n",
    "defaultでは$\\sigma=\\frac{D}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. scoreの再計算\n",
    "\n",
    "<img src ='img/scorerecalc.png' width =400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.以下同様...\n",
    "\n",
    "\n",
    "### $$ c_t = \\sum_s \\alpha_t(s)\\bar{h_s} $$\n",
    "\n",
    "### $$ \\tilde{h_t} = tanh({W_c[c_t;h_t]}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書分類にはどうやってAttentionを適用させたらいいの？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.LSTM or GRU or RNN + Attention\n",
    "\n",
    "[Hierarchical Attention Networks for Document Classification(Yang et.al,NAACL2016])](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)　<br>\n",
    "を参考に実装\n",
    "\n",
    "通常のLSTMから文書分類を行う場合は最終時刻の結果が用いられる。\n",
    "\n",
    "Attentionを用いる場合は今までと同様、どの時刻の隠れ層に着目すべきかの加重和でcontext vectorを生成する。\n",
    "\n",
    "しかし今回いままでと対応するDecoder側での$h_t$と対応するものって何...?\n",
    "\n",
    "\n",
    "**$h_t$と対応するものはランダムに初期化した学習していく重み**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/attention.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.スコアの計算\n",
    "$i$は$0$〜$n$<br>\n",
    "$W_s,b_s$は学習させる重み\n",
    "### $$ u_i = tanh(W_sh_i + b_s) $$\n",
    "\n",
    "$u_s$が前までのDecoder側での$h_t$の代わりで、学習する重み\n",
    "### $$ score(u_i,u_s) = u^T_i u_s $$\n",
    "\n",
    "### 2.softmaxで正規化\n",
    "### $$α_i = \\frac{exp(score(u_i,u_s) )}{\\sum_{i'} exp(score(u_{i'},u_s) )}$$\n",
    "\n",
    "### 3.加重和\n",
    "### $$v =\\sum_i α_ih_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Bidirectional, Dense,Merge,RepeatVector,Multiply,Lambda\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Input, Embedding, SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_16 (InputLayer)            (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Emb (Embedding)                  (None, 100, 300)      300000      input_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "LSTM (LSTM)                      (None, 100, 128)      219648      Emb[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "T1 (TimeDistributed)             (None, 100, 32)       4128        LSTM[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "T2 (TimeDistributed)             (None, 100, 1)        33          T1[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 100, 1)        0           T2[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "attention_mul (Multiply)         (None, 100, 128)      0           activation_2[0][0]               \n",
      "                                                                   LSTM[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 128)           0           attention_mul[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 523,809\n",
      "Trainable params: 523,809\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# kerasを用いた実装\n",
    "\n",
    "input = Input(shape=(100,))\n",
    "embedding = Embedding(input_dim=1000, output_dim=300,name='Emb')(input)\n",
    "hs = LSTM(128, return_sequences=True,name='LSTM')(embedding)\n",
    "\n",
    "u=TimeDistributed(Dense(32,  activation='tanh'),name='T1')(hs)\n",
    "score = TimeDistributed(Dense(1),name='T2')(u)\n",
    "alpha=Activation('softmax')(score)\n",
    "alphahs=Multiply(name='attention_mul')([alpha,hs])\n",
    "\n",
    "v = Lambda(lambda x: K.sum(x, axis=1))(alphahs)\n",
    "\n",
    "model = Model(inputs=input, outputs=v)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kerasを用いたAttenionLayerの実装\n",
    "Download from here\n",
    "\n",
    "https://github.com/richliao/textClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Attention Networks for Document Classification(Yang et.al,NAACL2016])\n",
    "https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\n",
    "\n",
    "\n",
    "文書分類（sentence classificationじゃない。複数文入った文書）についてAttentionを取り入れた\n",
    "\n",
    "二つのレベルのAttention\n",
    "- 単語レベルのAttention(これだけ使ってSentence Classificationしてもいい)\n",
    "- 文書レベルのAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Attention Networks(HAN)\n",
    "<img src ='img/hierarchicl.png' width =600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.双方向のGRU+Attentionで１文ごとにEncode\n",
    "### 2.Encodeされた文の順番でもう一度双方向GRU+Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attentionの可視化\n",
    "<img src='img/attentionvisual.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この仕組みを使えばtextCNNにもQRNNにもAttention機構が加えられる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_15 (InputLayer)            (None, 56)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 56, 256)       4803840     input_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 56, 256, 1)    0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 54, 1, 512)    393728      reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 53, 1, 512)    524800      reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 52, 1, 512)    655872      reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 1, 1, 512)     0           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)   (None, 1, 1, 512)     0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)   (None, 1, 1, 512)     0           conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 3, 1, 512)     0           max_pooling2d_4[0][0]            \n",
      "                                                                   max_pooling2d_5[0][0]            \n",
      "                                                                   max_pooling2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1536)          0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 1536)          0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 2)             3074        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,381,314\n",
      "Trainable params: 6,381,314\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "#sequence_length = x.shape[1] # 56\n",
    "#vocabulary_size = len(vocabulary_inv) # 18765\n",
    "sequence_length = 56\n",
    "vocabulary_size =  18765\n",
    "\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 30\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maxpoolをAttention層に書き換えなさい\n",
    "\n",
    "# WRITE ME\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
